#  Backup and Restore

## 1. We have a working kubernetes cluster with a set of applications running. Let us first explore the setup.

How many deployments exist in the cluster?

```bash
controlplane ~ ➜  kubectl get deployments
NAME   READY   UP-TO-DATE   AVAILABLE   AGE
blue   3/3     3            3           13s
red    2/2     2            2           13s
```

## 2. What is the version of ETCD running on the cluster?

Check the ETCD Pod or Process. Look at the ETCD Logs OR check the image used by ETCD pod
Solution: Look at the ETCD Logs using the command ``kubectl logs etcd-controlplane -n kube-system`` or check the image used by the ETCD pod: ``kubectl describe pod etcd-controlplane -n kube-system``
```bash
controlplane ~ ✖ kubectl logs etcd-controlplane -n kube-system
```

```bash
controlplane ~ ✖ kubectl get pod -n kube-system
NAME                                   READY   STATUS    RESTARTS   AGE
coredns-787d4945fb-j75hl               1/1     Running   0          3m2s
coredns-787d4945fb-jkmcg               1/1     Running   0          3m2s
etcd-controlplane                      1/1     Running   0          3m15s
kube-apiserver-controlplane            1/1     Running   0          3m15s
kube-controller-manager-controlplane   1/1     Running   0          3m15s
kube-proxy-pbkd7                       1/1     Running   0          3m2s
kube-scheduler-controlplane            1/1     Running   0          3m18
```

```bash
controlplane ~ ➜  kubectl describe pod etcd-controlplane -n kube-system
```

```bash
controlplane ~ ➜  kubectl describe pod etcd-controlplane -n kube-system
---
Containers:
  etcd:
    Container ID:  containerd://6a6c89970fa6da03bf63942c8e41190e0a3c446e86dae3ae21257d2f5ad55b9c
    Image:         registry.k8s.io/etcd:3.5.6-0
    Image ID:      registry.k8s.io/etcd@sha256:dd75ec974b0a2a6f6bb47001ba09207976e625db898d1b16735528c009cb171c
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://192.10.150.9:2379
---
```
"3.5.6"

## 3. At what address can you reach the ETCD cluster from the controlplane node?

Check the ETCD Service configuration in the ETCD POD
**Hint**: Use the command ``kubectl describe pod etcd-controlplane -n kube-system`` and look for ``--listen-client-urls``
```bash
controlplane ~ ➜  kubectl -n kube-system describe pod etcd-controlplane | grep '\--listen-client-urls'
      --listen-client-urls=https://127.0.0.1:2379,https://192.10.150.9:2379
```
https://127.0.0.1:2379

## 4. Where is the ETCD server certificate file located?

Note this path down as you will need to use it later

**Hint**: Check the ETCD pod configuration ``kubectl describe pod etcd-controlplane -n kube-system``

Solution: 
Check the ETCD pod configuration with the command: ``kubectl describe pod etcd-controlplane -n kube-system ``and look for the value for ``--cert-file``:

```bash
controlplane ~ ➜  kubectl -n kube-system describe pod etcd-controlplane | grep '\--cert-file'
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
```
/etc/kubernetes/pki/etcd/server.crt

## 5. Where is the ETCD CA Certificate file located?

Note this path down as you will need to use it later.
```bash
controlplane ~ ➜  kubectl -n kube-system describe pod etcd-controlplane | grep '\--trusted-ca-file'
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.cr
```
/etc/kubernetes/pki/etcd/ca.crt

## 6. The master node in our cluster is planned for a regular maintenance reboot tonight. While we do not anticipate anything to go wrong, we are required to take the necessary backups. Take a snapshot of the ETCD database using the built-in snapshot functionality.

Store the backup file at location /opt/snapshot-pre-boot.db
- Backup ETCD to /opt/snapshot-pre-boot.db

**Hint**: 
Use the ``etcdctl snapshot save`` command. You will have to make use of additional flags to connect to the ETCD server.

``--endpoints``: Optional Flag, points to the address where ETCD is running (127.0.0.1:2379)

``--cacert``: Mandatory Flag (Absolute Path to the CA certificate file)

``--cert``: Mandatory Flag (Absolute Path to the Server certificate file)

``--key``: Mandatory Flag (Absolute Path to the Key file)
```bash
controlplane ~ ➜  ls /etc/kubernetes/manifests
etcd.yaml  kube-apiserver.yaml  kube-controller-manager.yaml  kube-scheduler.yaml
```

```yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/etcd.advertise-client-urls: https://192.10.150.9:2379
  creationTimestamp: null
  labels:
    component: etcd
    tier: control-plane
  name: etcd
  namespace: kube-system
spec:
  containers:
  - command:
    - etcd
    - --advertise-client-urls=https://192.10.150.9:2379
    - --cert-file=/etc/kubernetes/pki/etcd/server.crt # cert
    - --client-cert-auth=true
    - --data-dir=/var/lib/etcd
    - --experimental-initial-corrupt-check=true
    - --experimental-watch-progress-notify-interval=5s
    - --initial-advertise-peer-urls=https://192.10.150.9:2380
    - --initial-cluster=controlplane=https://192.10.150.9:2380
    - --key-file=/etc/kubernetes/pki/etcd/server.key # key
    - --listen-client-urls=https://127.0.0.1:2379,https://192.10.150.9:2379 # link
    - --listen-metrics-urls=http://127.0.0.1:2381
    - --listen-peer-urls=https://192.10.150.9:2380
    - --name=controlplane
    - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
    - --peer-client-cert-auth=true
    - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
    - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    - --snapshot-count=10000
    - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt # cacert
    image: registry.k8s.io/etcd:3.5.6-0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 127.0.0.1
        path: /health?exclude=NOSPACE&serializable=true
        port: 2381
        scheme: HTTP
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    name: etcd
    resources:
      requests:
        cpu: 100m
        memory: 100Mi
    startupProbe:
      failureThreshold: 24
      httpGet:
        host: 127.0.0.1
        path: /health?serializable=false
        port: 2381
        scheme: HTTP
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    volumeMounts:
    - mountPath: /var/lib/etcd
      name: etcd-data
    - mountPath: /etc/kubernetes/pki/etcd
      name: etcd-certs
  hostNetwork: true
  priorityClassName: system-node-critical
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - hostPath:
      path: /etc/kubernetes/pki/etcd
      type: DirectoryOrCreate
    name: etcd-certs
  - hostPath:
      path: /var/lib/etcd
      type: DirectoryOrCreate
    name: etcd-data
status: {}
```
https://github.com/mmumshad/kubernetes-the-hard-way/blob/master/practice-questions-answers/cluster-maintenance/backup-etcd/etcd-backup-and-restore.md

```bash
controlplane ~ ➜  ls /var/lib/etcd
member
```

```bash
controlplane ~ ➜  ls /etc/kubernetes/pki/etcd
ca.crt  healthcheck-client.crt  peer.crt  server.crt
ca.key  healthcheck-client.key  peer.key  server.key
```

```bash
controlplane ~ ➜  ETCDCTL_API=3 etcdctl snapshot

controlplane ~ ➜  export ETCDCTL_API=3

controlplane ~ ➜  etcdctl snapshot

```

```bash
controlplane ~ ➜  ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 \
> --cacert=/etc/kubernetes/pki/etcd/ca.crt \
> --cert=/etc/kubernetes/pki/etcd/server.crt \
> --key=/etc/kubernetes/pki/etcd/server.key \
> snapshot save /opt/snapshot-pre-boot.db
Snapshot saved at /opt/snapshot-pre-boot.db
```

## 7. Great! Let us now wait for the maintenance window to finish. Go get some sleep. (Don't go for real)


Click Ok to Continue

## 8. Wake up! We have a conference call! After the reboot the master nodes came back online, but none of our applications are accessible. Check the status of the applications on the cluster. What's wrong?
```bash
controlplane ~ ➜  k get svc
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   21s
```
Services are not present

Deployments are not present

Pods are not present

All of the above: TRUE

## 9. Luckily we took a backup. Restore the original state of the cluster using the backup file.
- Deployments: 2
- Services: 3

**Hint**: Restore the etcd to a new directory from the snapshot by using the ``etcdctl snapshot restore`` command. Once the directory is restored, update the ETCD configuration to use the restored directory.

Solution: 

**First Restore the snapshot:**

```bash
controlplane ~ ✖ ETCDCTL_API=3 etcdctl  --data-dir /var/lib/etcd-from-backup \
> snapshot restore /opt/snapshot-pre-boot.db
2023-04-29 09:39:14.827008 I | mvcc: restore compact to 1669
2023-04-29 09:39:14.896605 I | etcdserver/membership: added member 8e9e05c52164694d [http://localhost:2380] to cluster cdf818194e3a8c32
```
or 
```bash
etcdctl snapshot --data-dir /var/lib/etcd-from-backup \
> snapshot restore /opt/snapshot-pre-boot.db
```

```bash
controlplane ~ ✖ ls /var/lib/etcd-from-backup
member
```
Note: In this case, we are restoring the snapshot to a different directory but in the same server where we took the backup (the controlplane node) As a result, the only required option for the restore command is the ``--data-dir``.


Next, update the ``/etc/kubernetes/manifests/etcd.yaml``:

We have now restored the etcd snapshot to a new path on the controlplane -`` /var/lib/etcd-from-backup``, so, the only change to be made in the YAML file, is to change the hostPath for the volume called ``etcd-data`` from old directory (``/var/lib/etcd``) to the new directory (``/var/lib/etcd-from-backup``).
```yaml
  volumes:
  - hostPath:
      path: /var/lib/etcd-from-backup
      type: DirectoryOrCreate
    name: etcd-data
```
With this change, ``/var/lib/etcd`` on the container points to ``/var/lib/etcd-from-backup`` on the controlplane (which is what we want).

When this file is updated, the ``ETCD`` pod is automatically re-created as this is a static pod placed under the ``/etc/kubernetes/manifests`` directory.


- Note 1: As the ETCD pod has changed it will automatically restart, and also ``kube-controller-manager`` and kube-scheduler. Wait 1-2 to mins for this pods to restart. You can run the command: ``watch "crictl ps | grep etcd"`` to see when the ETCD pod is restarted.

- Note 2: If the etcd pod is not getting ``Ready 1/1``, then restart it by ``kubectl delete pod -n kube-system etcd-controlplane`` and wait 1 minute.

- Note 3: This is the simplest way to make sure that ETCD uses the restored data after the ETCD pod is recreated. You don't have to change anything else.


If you do change ``--data-dir`` to ``/var/lib/etcd-from-backup`` in the ETCD YAML file, make sure that the ``volumeMounts`` for ``etcd-data`` is updated as well, with the mountPath pointing to ``/var/lib/etcd-from-backup`` (THIS COMPLETE STEP IS OPTIONAL AND NEED NOT BE DONE FOR COMPLETING THE RESTORE)

Note: --data and volume mountpoint match all the time 
```bash
controlplane ~ ➜  k get pods -n kube-system
NAME                                   READY   STATUS    RESTARTS        AGE
coredns-787d4945fb-j75hl               1/1     Running   0               37m
coredns-787d4945fb-jkmcg               1/1     Running   0               37m
etcd-controlplane                      0/1     Pending   0               3m32s
kube-apiserver-controlplane            1/1     Running   1 (3m40s ago)   37m
kube-controller-manager-controlplane   1/1     Running   2 (5m50s ago)   37m
kube-proxy-pbkd7                       1/1     Running   0               37m
kube-scheduler-controlplane            1/1     Running   2 (5m45s ago)   37m
```
look etcd-controlplane                      0/1     Pending   0               3m32s
```bash
```
```bash
```
```bash
```
```bash
```
```bash
```
```bash
```
```bash
```
