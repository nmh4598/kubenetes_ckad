#  OS Upgrades

## 1. Let us explore the environment first. How many nodes do you see in the cluster?

Including the controlplane and worker nodes.

```bash
controlplane ~ ➜  k get nodes 
NAME           STATUS   ROLES           AGE     VERSION
controlplane   Ready    control-plane   5m38s   v1.26.0
node01         Ready    <none>          5m6s    v1.26.0
```
2

## 2. How many applications do you see hosted on the cluster?

Check the number of deployments in the default namespace.
```bash
controlplane ~ ➜  k get deploy
NAME   READY   UP-TO-DATE   AVAILABLE   AGE
blue   3/3     3            3           21s

```
1

## 3. Which nodes are the applications hosted on?
```bash
controlplane ~ ➜  k get pod
NAME                   READY   STATUS    RESTARTS   AGE
blue-987f68cb5-28mfr   1/1     Running   0          24s
blue-987f68cb5-84bd5   1/1     Running   0          24s
blue-987f68cb5-lwxdk   1/1     Running   0          24s

controlplane ~ ➜  k get pod -o wide
NAME                   READY   STATUS    RESTARTS   AGE   IP           NODE           NOMINATED NODE   READINESS GATES
blue-987f68cb5-28mfr   1/1     Running   0          75s   10.244.1.3   node01         <none>           <none>
blue-987f68cb5-84bd5   1/1     Running   0          75s   10.244.1.2   node01         <none>           <none>
blue-987f68cb5-lwxdk   1/1     Running   0          75s   10.244.0.4   controlplane   <none>           <none>
```
controlplane,node01

## 4. We need to take node01 out for maintenance. Empty the node of all applications and mark it unschedulable.
- Node node01 Unschedulable
- Pods evicted from node01

```bash
controlplane ~ ➜  k drain node01
node/node01 cordoned
error: unable to drain node "node01" due to error:cannot delete DaemonSet-managed Pods (use --ignore-daemonsets to ignore): kube-flannel/kube-flannel-ds-9gfjt, kube-system/kube-proxy-29f98, continuing command...
There are pending nodes to be drained:
 node01
cannot delete DaemonSet-managed Pods (use --ignore-daemonsets to ignore): kube-flannel/kube-flannel-ds-9gfjt, kube-system/kube-proxy-29f98

controlplane ~ ✖ kubectl drain node01 --ignore-daemonsets
node/node01 already cordoned
Warning: ignoring DaemonSet-managed Pods: kube-flannel/kube-flannel-ds-9gfjt, kube-system/kube-proxy-29f98
evicting pod default/blue-987f68cb5-84bd5
evicting pod default/blue-987f68cb5-28mfr
pod/blue-987f68cb5-84bd5 evicted
pod/blue-987f68cb5-28mfr evicted
node/node01 drained
```

## 5. What nodes are the apps on now?
before drain 

```bash
controlplane ~ ➜  k get pod -o wide
NAME                   READY   STATUS    RESTARTS   AGE   IP           NODE           NOMINATED NODE   READINESS GATES
blue-987f68cb5-28mfr   1/1     Running   0          75s   10.244.1.3   node01         <none>           <none>
blue-987f68cb5-84bd5   1/1     Running   0          75s   10.244.1.2   node01         <none>           <none>
blue-987f68cb5-lwxdk   1/1     Running   0          75s   10.244.0.4   controlplane   <none>           <none>

controlplane ~ ➜  k get nodes
NAME           STATUS                     ROLES           AGE   VERSION
controlplane   Ready                      control-plane   21m   v1.26.0
node01         Ready,SchedulingDisabled   <none>          20m   v1.26.0
```
after drain 

```bash
controlplane ~ ➜  k get pod -o wide
NAME                   READY   STATUS    RESTARTS   AGE     IP           NODE           NOMINATED NODE   READINESS GATES
blue-987f68cb5-jtxch   1/1     Running   0          8m38s   10.244.0.5   controlplane   <none>           <none>
blue-987f68cb5-lwxdk   1/1     Running   0          15m     10.244.0.4   controlplane   <none>           <none>
blue-987f68cb5-z2rv4   1/1     Running   0          8m38s   10.244.0.6   controlplane   <none>           <none>
```
## 6. The maintenance tasks have been completed. Configure the node node01 to be schedulable again.
- Node01 is Schedulable
```bash
controlplane ~ ➜  kubectl uncordon node01
node/node01 uncordoned
```

## 7. How many pods are scheduled on node01 now?

```bash
controlplane ~ ➜  kubectl get pods -o wide
NAME                   READY   STATUS    RESTARTS   AGE   IP           NODE           NOMINATED NODE   READINESS GATES
blue-987f68cb5-jtxch   1/1     Running   0          11m   10.244.0.5   controlplane   <none>           <none>
blue-987f68cb5-lwxdk   1/1     Running   0          19m   10.244.0.4   controlplane   <none>           <none>
blue-987f68cb5-z2rv4   1/1     Running   0          11m   10.244.0.6   controlplane   <none>           <none>
```
0
## 8. Why are there no pods on node01?

node01 did not upgrade successfully XXX

node01 is cordoned XXX

node01 is faulty XXX

Only when new pods are created they will be scheduled 

## 9. Why are the pods placed on the controlplane node?

controlplane node has taints set on it XXX

controlplane node is faulty XXX

controlplane node does not have any taints

controlplane node is cordoned XXX

you can never have pods on master nodes
Check the controlplane node details.XXX

Note : Usually we dont we pod in controlplane because they have taints 
## 10. 
```bash
controlplane ~ ➜  k describe node controlplane
---
Taints:             <none>
--- 
```

## 11. We need to carry out a maintenance activity on node01 again. Try draining the node again using the same command as before: kubectl drain node01 --ignore-daemonsets

Did that work?

NO
```bash
controlplane ~ ➜  kubectl drain node01 --ignore-daemonsets
node/node01 cordoned
error: unable to drain node "node01" due to error:cannot delete Pods declare no controller (use --force to override): default/hr-app, continuing command...
There are pending nodes to be drained:
 node01
cannot delete Pods declare no controller (use --force to override): default/hr-app
```

## 12. Why did the drain command fail on node01? It worked the first time!

node01 was not upgraded correctly the last time XX

no pods on node01 XX

node01 tainted XX

there is a pod in node01 which is not part of a replicaset
```bash
controlplane ~ ✖ k get pods
NAME                   READY   STATUS    RESTARTS   AGE
blue-987f68cb5-jtxch   1/1     Running   0          18m
blue-987f68cb5-lwxdk   1/1     Running   0          25m
blue-987f68cb5-z2rv4   1/1     Running   0          18m
hr-app                 1/1     Running   0          117s

controlplane ~ ➜  k get pod -o wide
NAME                   READY   STATUS    RESTARTS   AGE     IP           NODE           NOMINATED NODE   READINESS GATES
blue-987f68cb5-jtxch   1/1     Running   0          19m     10.244.0.5   controlplane   <none>           <none>
blue-987f68cb5-lwxdk   1/1     Running   0          26m     10.244.0.4   controlplane   <none>           <none>
blue-987f68cb5-z2rv4   1/1     Running   0          19m     10.244.0.6   controlplane   <none>           <none>
hr-app                 1/1     Running   0          2m49s   10.244.1.4   node01         <none>           <none>
```
 hr-app   new addition which is a pod that has been added 

It is easy to delete pod from Replicationcontroller, job , daemonSet, Statefulset because they take care of recreating the pod on another node. So risky of draining process, you can just go ahead and kill the pods. Then replicaset or whatever, it is managing the pod would take care of creating on other node. in this case, only pod, it is not one that is created as part of a deployment or replicaset or any of this. If we delete hr-app pod as part of this training process, when recreat or store locally on that pod is going to be lost forever so error happen 

## 13. What is the name of the POD hosted on node01 that is not part of a replicaset?

hr-app

blue XX

redis XX

red XX

simple-webapp-1 XX

## 14. What would happen to hr-app if node01 is drained forcefully?

Try it and see for yourself.

A forceful drain of the node will delete any pod that is not part of a replicaset.

```bash
```
Going to be deleted hr-app will be lost forever

## 15. Oops! We did not want to do that! hr-app is a critical application that should not be destroyed. We have now reverted back to the previous state and re-deployed hr-app as a deployment.
```bash
controlplane ~ ➜  k get pod -o wide
NAME                      READY   STATUS    RESTARTS   AGE   IP           NODE           NOMINATED NODE   READINESS GATES
blue-987f68cb5-jtxch      1/1     Running   0          28m   10.244.0.5   controlplane   <none>           <none>
blue-987f68cb5-lwxdk      1/1     Running   0          35m   10.244.0.4   controlplane   <none>           <none>
blue-987f68cb5-z2rv4      1/1     Running   0          28m   10.244.0.6   controlplane   <none>           <none>
hr-app-66c4c9c67f-ggvg9   1/1     Running   0          18s   10.244.1.5   node01         <none>           <none>
```


## 16. hr-app is a critical app and we do not want it to be removed and we do not want to schedule any more pods on node01.
Mark node01 as unschedulable so that no new pods are scheduled on this node.

Make sure that hr-app is not affected.
- Node01 Unschedulable
- hr-app still running on node01?

**Hint**: Run the command kubectl cordon node01

Do not drain node01, instead use the kubectl cordon node01 command. This will ensure that no new pods are scheduled on this node and the existing pods will not be affected by this operation.

```bash
controlplane ~ ➜  kubectl cordon node01
node/node01 cordoned

controlplane ~ ➜  k get nodes
NAME           STATUS                     ROLES           AGE   VERSION
controlplane   Ready                      control-plane   42m   v1.26.0
node01         Ready,SchedulingDisabled   <none>          42m   v1.26.0

```

```bash
controlplane ~ ➜  k get pod -o wide
NAME                      READY   STATUS    RESTARTS   AGE   IP           NODE           NOMINATED NODE   READINESS GATES
blue-987f68cb5-jtxch      1/1     Running   0          30m   10.244.0.5   controlplane   <none>           <none>
blue-987f68cb5-lwxdk      1/1     Running   0          37m   10.244.0.4   controlplane   <none>           <none>
blue-987f68cb5-z2rv4      1/1     Running   0          30m   10.244.0.6   controlplane   <none>           <none>
hr-app-66c4c9c67f-ggvg9   1/1     Running   0          95s   10.244.1.5   node01         <none>           <none>
```

```bash
```

```bash
```

```bash
```

```bash
```

```bash
```

```bash
```

```bash
```