## 12. SECTION: SCHEDULING
For this question, please set the context to ``cluster1`` by running:
```bash
kubectl config use-context cluster1
```
The deployment called ``trace-wl08`` inside the ``test-wl08`` namespace on cluster1 has undergone several, routine, rolling updates and rollbacks.


Inspect the ``revision 2`` of this deployment and store the image name that was used in this revision in the ``/opt/trace-wl08-revision-book.txt`` file on the ``student-node``.

Solution 

Set the correct context: -
```bash
kubectl get deploy -n test-wl08
```
List the deployments in ``test-wl08`` namespace as follows: -
```bash
kubectl get deploy -n test-wl08
```
Run the following command to check the revisions of the deployment called ``trace-wl08:-``
```bash
kubectl rollout history deployment -n test-wl08 trace-wl08
```
Inspect the ``revision 2`` by using the ``--revision`` option: -
```bash
kubectl rollout history deployment -n test-wl08 trace-wl08 --revision=2
```
Under the ``Containers`` section, you will see the ``image`` name.

On the ``student-node``, save that image name in the given file`` /opt/trace-wl08-revision-book.txt:``
```bash
echo "busybox:1.35" > /opt/trace-wl08-revision-book.txt
```
## 13. SECTION: SCHEDULING
For this question, please set the context to ``cluster3`` by running:
```bash
kubectl config use-context cluster3
```

One of our applications runs on the ``cluster3-controlplane`` node. Due to the possibility of traffic increase, we want to scale the application pods to loadbalance the traffic and provide a smoother user experience.

``cluster3-controlplane`` node has enough resources to deploy more application pods. Scale the deployment called ``essports-wl02`` to ``5``

Solution

Set the correct context:
```bash
kubectl config use-context cluster3
```
Now, get the details of the nodes: -
```bash
kubectl get nodes -o wide
```
then ``SSH`` to the given node by the following command: -
```bash
ssh cluster3-controlplane
```
And run the ``kubectl scale`` command as follows: -
```bash
kubectl scale deploy essports-wl02 --replicas=5
```
OR

You can run the ``kubectl scale`` command from the ``student`` node as well: -
```bash
kubectl scale deploy essports-wl02 --replicas=5
```
Verify the scaled-up pods by ``kubectl get`` command: -
```bash
kubectl get pods
```
The number of pods should be 1 to 5.

## 14.SECTION: SCHEDULING

For this question, please set the context to cluster3 by running:

```bash
kubectl config use-context cluster3
```
One of our Junior DevOps engineers have deployed a pod ``nginx-wl06`` on the ``cluster3-controlplane`` node. However, while specifying the resource limits, instead of using ``Mebibyte`` as the unit, ``Gebibyte`` was used.

As a result, the node doesn't have sufficient resources to deploy this pod and it is stuck in a pending state

Fix the units and re-deploy the pod (Delete and recreate the pod if needed).

Solution

Set the correct context: -

```bash
kubectl config use-context cluster3
```
Run the following command to check the pending pods on all the namespaces: -
```bash
kubectl get pods -A
```
After that, inspect the pod Events as follows: -

```bash
kubectl get pods -A | grep -i pending

kubectl describe po nginx-wl06
```
Make use of the ``kubectl edit`` command to update the values from ``Gi`` to ``Mi``:-
```bash
kubectl edit po nginx-wl06
```
It will save the temporary file under the ``/tmp/`` directory. Use the ``kubectl replace`` command as follows: -
```bash
kubectl replace -f /tmp/kubectl-edit-xxxx.yaml --force
```
It will delete the existing pod and will re-create it again with new changes.
## 15. SECTION: STORAGE

For this question, please set the context to cluster1 by running:
SECTION: STORAGE

For this question, please set the context to cluster1 by running:

There is a requirement to share a volume between two containers that are running within the same pod. Use the following instructions to create the pod and related objects:


- Create a pod named ``grape-pod-cka06-str``.

- The main container should use the nginx image and mount a volume called ``grape-vol-cka06-str ``at path ``/var/log/nginx``.

- The sidecar container can use ``busybox`` image, you might need to add a ``sleep`` command to this container to keep it running. Next, mount the same volume called ``grape-vol-cka06-str`` at the path ``/usr/src.``

- The volume should be of type ``emptyDir``.

Solution

Set context to ``cluster``.

**Create a yaml file as below:**

```bash
apiVersion: v1
kind: Pod
metadata:
  name: grape-pod-cka06-str
spec:
  containers:
  - name: nginx
    image: nginx
    volumeMounts:
      - name: grape-vol-cka06-str
        mountPath: "/var/log/nginx"
  - name: busybox
    image: busybox
    command:
      - "bin/sh"
      - "-c"
      - "sleep 10000"
    volumeMounts:
      - name: grape-vol-cka06-str
        mountPath: "/usr/src"
  volumes:
  - name: grape-vol-cka06-str
    emptyDir: {}
```
**Apply the template:**
```bash
kubectl apply -f <template-file-name>.yaml
```
## 16. SECTION: STORAGE

There is a`` persistent volume`` named ``apple-pv-cka04-str``. Create a ``persistent volume`` claim named ``apple-pvc-cka04-str`` and request a 40Mi of storage from ``apple-pv-cka04-str`` PV.
The access mode should be ``ReadWriteOnce`` and storage class should be ``manual``.

Solution
Set context to cluster1:

**Create a yaml template as below:**
```bash
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: apple-pvc-cka04-str
spec:
  volumeName: apple-pv-cka04-str
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 40Mi
```
A**pply the template:**
```bash
kubectl apply -f <template-file-name>.yaml
```