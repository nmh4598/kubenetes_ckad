## 1 SECTION: ARCHITECTURE, INSTALL AND MAINTENANCE
For this question, please set the context to cluster1 by running:

    kubectl config use-context cluster1

Create a service account called `pink-sa-cka24-arch`. Further create a cluster role called `pink-role-cka24-arch` with `full permissions` on all resources in the core api group under `default` namespace in `cluster1`.


Finally create a cluster role binding called `pink-role-binding-cka24-arch` to bind `pink-role-cka24-arch` cluster role with `pink-sa-cka24-arch` service account.

```bash
student-node ~ ➜ kubectl --context cluster1 create serviceaccount pink-sa-cka24-arch
student-node ~ ➜ kubectl --context cluster1 create clusterrole pink-role-cka24-arch --resource=* --verb=*
student-node ~ ➜ kubectl --context cluster1 create clusterrolebinding pink-role-binding-cka24-arch --clusterrole=pink-role-cka24-arch --serviceaccount=default:pink-sa-cka24-arch
```

## 2 SECTION: ARCHITECTURE, INSTALL AND MAINTENANCE

Find the node across all clusters that consumes the most CPU and store the result to the file `/opt/high_cpu_node` in the following format `cluster_name,node_name`.

The node could be in any clusters that are currently configured on the `student-node`.

Solution
Check out the metrics for all node across all clusters:

```bash
student-node ~ ➜  kubectl top node --context cluster1 --no-headers | sort -nr -k2 | head -1
cluster1-controlplane   127m   1%    703Mi   1%    

student-node ~ ➜  kubectl top node --context cluster2 --no-headers | sort -nr -k2 | head -1
cluster2-controlplane   126m   1%    675Mi   1%    

student-node ~ ➜  kubectl top node --context cluster3 --no-headers | sort -nr -k2 | head -1
cluster3-controlplane   577m   7%    1081Mi   1%    

student-node ~ ➜  kubectl top node --context cluster4 --no-headers | sort -nr -k2 | head -1
cluster4-controlplane   130m   1%    679Mi   1%    

```
Using this, find the node that uses most `cpu`. In this case, it is `cluster3-controlplane` on `cluster3`.


Save the result in the correct format to the file:
```bash
student-node ~ ➜  echo cluster3,cluster3-controlplane > /opt/high_cpu_node
```

## 3 SECTION: ARCHITECTURE, INSTALL AND MAINTENANCE

For this question, please set the context to cluster1 by running:

    kubectl config use-context cluster1

We have created a service account called `blue-sa-cka21-arch`, a cluster role called `blue-role-cka21-arch` and a cluster role binding called `blue-role-binding-cka21-arch`.


Update the permissions of this service account so that it can get the `pods` only in `default` namespace of `cluster1`.

Solution
Edit the blue-role-cka21-arch to update permissions:

```bash
student-node ~ ➜  kubectl edit clusterrole blue-role-cka21-arch --context cluster1
```
Under `rules:` -> `- apiGroups:` replace `apps` with `""`

`resources:` replace `- deployments` with `- pods` and save the changes.

You can verify it as below:
```bash
student-node ~ ➜  kubectl auth can-i get pods 
--as=system:serviceaccount:default:blue-sa-cka21-arch
yes

```

## 4 SECTION: ARCHITECTURE, INSTALL AND MAINTENANCE
For this question, please set the context to `cluster3` by running:

```bash
kubectl config use-context cluster3
```
A pod called `logger-cka03-arch` has been created in the default namespace. Inspect this pod and save ALL `INFO` and ERROR's to the file `/root/logger-cka03-arch-all` on the `student-node`.

Solution

Run the command `kubectl logs logger-cka03-arch --context cluster3 > /root/logger-cka03-arch-all` on the `student-node`.

Run the command :
```bash
student-node ~ ➜ kubectl logs logger-cka03-arch --context cluster3 > /root/logger-cka03-arch-all

student-node ~ ➜  head /root/logger-cka03-arch-all
INFO: Wed Oct 19 10:38:57 UTC 2022 Logger is running
ERROR: Wed Oct 19 10:38:57 UTC 2022 Logger encountered errors!
INFO: Wed Oct 19 10:38:57 UTC 2022 Logger is running
ERROR: Wed Oct 19 10:38:57 UTC 2022 Logger encountered errors!
INFO: Wed Oct 19 10:38:57 UTC 2022 Logger is running
ERROR: Wed Oct 19 10:38:57 UTC 2022 Logger encountered errors!
INFO: Wed Oct 19 10:38:57 UTC 2022 Logger is running
ERROR: Wed Oct 19 10:38:57 UTC 2022 Logger encountered errors!
INFO: Wed Oct 19 10:38:57 UTC 2022 Logger is running
ERROR: Wed Oct 19 10:38:57 UTC 2022 Logger encountered errors!

student-node ~ ➜  
```

## 5 SECTION: ARCHITECTURE, INSTALL AND MAINTENANCE

Find the node across all clusters that consumes the most `memory` and store the result to the file `/opt/high_memory_node` in the following format `cluster_name,node_name`.

The node could be in any clusters that are currently configured on the `student-node`.

Solution

```bash
student-node ~ ➜  kubectl top node --context cluster1 --no-headers | sort -nr -k4 | head -1
cluster1-controlplane   124m   1%    768Mi   1%    

student-node ~ ➜  kubectl top node --context cluster2 --no-headers | sort -nr -k4 | head -1
cluster2-controlplane   79m   0%    873Mi   1%    

student-node ~ ➜  kubectl top node --context cluster3 --no-headers | sort -nr -k4 | head -1
cluster3-controlplane   78m   0%    902Mi   1%  

student-node ~ ➜  kubectl top node --context cluster4 --no-headers | sort -nr -k4 | head -1
cluster4-controlplane   78m   0%    901Mi   1%    

student-node ~ ➜  
```
Using this, find the node that uses most memory. In this case, it is `cluster3-controlplane` on cluster3.


Save the result in the correct format to the file:
```bash
student-node ~ ➜  echo cluster3,cluster3-controlplane > /opt/high_memory_node 
```

## 6. For this question, please set the context to cluster1 by running:

kubectl config use-context cluster1

A pod named beta-pod-cka01-arch has been created in the beta-cka01-arch namespace. Inspect the logs and save all logs starting with the string ERROR in file /root/beta-pod-cka01-arch_errors on the student-node.

Solution: 

Run the below commands:
```bash
student-node ~ ➜  kubectl -n beta-cka01-arch logs beta-pod-cka01-arch --context cluster1 | grep ERROR > /root/beta-pod-cka01-arch_errors

student-node ~ ➜  head /root/beta-pod-cka01-arch_errors
ERROR: Sat Jul 23 02:49:28 UTC 2022 Logger encountered errors!
ERROR: Sat Jul 23 02:49:32 UTC 2022 Logger encountered errors!
ERROR: Sat Jul 23 02:49:36 UTC 2022 Logger encountered errors!
ERROR: Sat Jul 23 02:49:40 UTC 2022 Logger encountered errors!
ERROR: Sat Jul 23 02:49:44 UTC 2022 Logger encountered errors!
ERROR: Sat Jul 23 02:49:48 UTC 2022 Logger encountered errors!
ERROR: Sat Jul 23 02:49:52 UTC 2022 Logger encountered errors!
ERROR: Sat Jul 23 02:49:56 UTC 2022 Logger encountered errors!
ERROR: Sat Jul 23 02:50:00 UTC 2022 Logger encountered errors!
ERROR: Sat Jul 23 02:50:04 UTC 2022 Logger encountered errors!

student-node ~ ➜ 
```

## 7. SECTION: ARCHITECTURE, INSTALL AND MAINTENANCE
```bash
kubectl config use-context cluster3
```
Run a pod called ``alpine-sleeper-cka15-arch`` using the alpine image in the ``default`` namespace that will sleep for ``7200`` seconds.

Solution

Create the pod definition:
```bash
apiVersion: v1
kind: Pod
metadata:
  name: alpine-sleeper-cka15-arch
spec:
  containers:
  - name: alpine
    image: alpine
    command: ["/bin/sh", "-c", "sleep 7200"]
```
Create the pod:
```bash
student-node ~ ➜  kubectl apply -f alpine-sleeper-cka15-arch.yaml --context cluster3
```
## 8. SECTION: ARCHITECTURE, INSTALL AND MAINTENANCE

For this question, please set the context to ``cluster3 ``by running:

```bash
kubectl config use-context cluster3
```
Decode the existing secret called beta-sec-cka14-arch created in the ``beta-ns-cka14-arch`` namespace and store the decoded content inside the file /opt/``beta-sec-cka14-arch`` on the ``student-node``.

Solution: 

Look for the data in ``beta-sec-cka14-arch`` secret:
```bash
student-node ~ ➜  kubectl get secret beta-sec-cka14-arch -o json --context cluster3 -n beta-ns-cka14-arch
```

You will find only one data entry in it called ``secret`` . Let's decode it and save output in`` /opt/beta-sec-cka14-arch`` file:
```bash
student-node ~ ➜  kubectl get secret beta-sec-cka14-arch --context cluster3 -n beta-ns-cka14-arch -o json | jq .data.secret | tr -d '"' | base64 -d > /opt/beta-sec-cka14-arch
```

## 9. SECTION: ARCHITECTURE, INSTALL AND MAINTENANCE

For this question, please set the context to ``cluster3`` by running:

```bash
kubectl config use-context cluster3
```
Run a pod called ``looper-cka16-arch`` using the busybox image that runs the while loop ``while true; do echo hello; sleep 10;done``. This pod should be created in the ``default`` namespace.

Solution: 

Create the pod definition:

```bash
student-node ~ ➜  vi looper-cka16-arch.yaml
```

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: looper-cka16-arch
spec:
  containers:
  - name: busybox
    image: busybox
    command: ["/bin/sh", "-c", "while true; do echo hello; sleep 10;done"]
```
Create the pod:

```bash
student-node ~ ➜  kubectl apply -f looper-cka16-arch.yaml --context cluster3
```

## 10. SECTION: ARCHITECTURE, INSTALL AND MAINTENANCE

For this question, please set the context to ``cluster1`` by running:

```bash
kubectl config use-context cluster1
```
Create a service account called ``deploy-cka20-arch``. Further create a cluster role called ``deploy-role-cka20-arch`` with permissions to ``get`` the ``deployments`` in ``default ``namespace on ``cluster1``.

Finally create a cluster role binding called ``deploy-role-binding-cka20-arch`` to bind ``deploy-role-cka20-arch`` cluster role with ``deploy-cka20-arch`` service account.

Solution: 

```bash
student-node ~ ➜  kubectl --context cluster1 create serviceaccount deploy-cka20-arch
student-node ~ ➜  kubectl --context cluster1 create clusterrole deploy-role-cka20-arch --resource=deployments --verb=get
student-node ~ ➜  kubectl --context cluster1 create clusterrolebinding deploy-role-binding-cka20-arch --clusterrole=deploy-role-cka20-arch --serviceaccount=default:deploy-cka20-arch
```
You can verify it as below:
```bash
student-node ~ ➜  kubectl --context cluster1 auth can-i get deployments --as=system:serviceaccount:default:deploy-cka20-arch
yes
```

## 11. SECTION: ARCHITECTURE, INSTALL AND MAINTENANCE
Create a generic secret called ``db-user-pass-cka17-arch`` in the ``default`` namespace on ``cluster1`` using the contents of the file ``/opt/db-user-pass`` on the ``student-node``
```bash
student-node ~ ➜  kubectl create secret generic db-user-pass-cka17-arch --from-file=/opt/db-user-pass
```

## 12 SECTION: ARCHITECTURE, INSTALL AND MAINTENANCE
We have created a service account called ``red-sa-cka23-arch``, a cluster role called ``red-role-cka23-arch`` and a cluster role binding called ``red-role-binding-cka23-arch``.


Identify the permissions of this service account and write down the answer in file ``/opt/red-sa-cka23-arch`` in format ``resource:pods|verbs:get,list`` on ``student-node``

Solution: 

Get the ``red-role-cka23-arch`` role permissions:
```bash
student-node ~ ➜  kubectl get clusterrole red-role-cka23-arch -o json --context cluster1

{
    "apiVersion": "rbac.authorization.k8s.io/v1",
    "kind": "ClusterRole",
    "metadata": {
        "creationTimestamp": "2022-10-20T07:16:39Z",
        "name": "red-role-cka23-arch",
        "resourceVersion": "16324",
        "uid": "e53cef4f-ae1b-49f7-b9fa-ac5e7e22a61c"
    },
    "rules": [
        {
            "apiGroups": [
                "apps"
            ],
            "resources": [
                "deployments"
            ],
            "verbs": [
                "get",
                "list",
                "watch"
            ]
        }
    ]
}
```
In this case, add data in file as below:
```bash
student-node ~ ➜ echo "resource:deployments|verbs:get,list,watch" > /opt/red-sa-cka23-arch
```

## 13. SECTION: ARCHITECTURE, INSTALL AND MAINTENANCE

``ETCD`` Key Value store which is running as a pod in ``cluster1`` . Take the backup of it and store it on the ``cluster1-controlplane`` node at the path ``/opt/cluster1_backup.db``.


You can ssh to the controlplane node by running ``ssh root@cluster1-controlplane`` from the ``student-node.``


NOTE: - If the ``etcd ``utility tool is unavailable on the controlplane, install it first.

Solution :

SSH into ``cluster1-controlplane`` node:
```bash
student-node ~ ➜ ssh root@cluster1-controlplane
```
Install ``etcd`` utility (if not installed already) and take ``etcd`` backup:
```bash
cluster1-controlplane ~ ➜ cd /tmp
cluster1-controlplane ~ ➜ export RELEASE=$(curl -s https://api.github.com/repos/etcd-io/etcd/releases/latest | grep tag_name | cut -d '"' -f 4)
cluster1-controlplane ~ ➜ wget https://github.com/etcd-io/etcd/releases/download/${RELEASE}/etcd-${RELEASE}-linux-amd64.tar.gz
cluster1-controlplane ~ ➜ tar xvf etcd-${RELEASE}-linux-amd64.tar.gz ; cd etcd-${RELEASE}-linux-amd64
cluster1-controlplane ~ ➜ mv etcd etcdctl  /usr/local/bin/
cluster1-controlplane ~ ➜ ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key snapshot save /opt/cluster1_backup.db
```

## 14. SECTION: ARCHITECTURE, INSTALL AND MAINTENANCE

Create a generic secret called ``secure-sec-cka12-arch`` in the ``secure-sys-cka12-arch`` namespace on the ``cluster3``. Use the key/value of ``color=darkblue`` to create the secret.

Solution:

Create a namespace called ``secure-sys-cka12-arch``
```bash
student-node ~ ➜  kubectl create ns secure-sys-cka12-arch --context cluster3
```
Create a generic secret called secure-sec-cka12-arch
```bash
student-node ~ ➜  kubectl create secret generic secure-sec-cka12-arch --from-literal=color=darkblue -n secure-sys-cka12-arch --context cluster3
```

## 15. SECTION: ARCHITECTURE, INSTALL AND MAINTENANCE

A pod called ``color-app-cka13-arch`` has been created in the ``default`` namespace. This pod logs can be accessed using ``kubectl logs -f color-app-cka13-arch`` command from the ``student-node``. It is currently displaying ``Color is pink`` output. Update the pod definition file to make use of the environment variable with the value - ``green`` and recreate this pod.

Solution:

Export the current pod definition:
```bash
student-node ~ ➜  kubectl get pod color-app-cka13-arch -o yaml > /tmp/color-app-cka13-arch.yaml
```
Edit the pod definition file to make the required changes:
```bash
student-node ~ ➜ vi /tmp/color-app-cka13-arch.yaml
```
Under ``env:`` -> - ``name:`` ``APP_COLOR`` change ``value: pink`` to ``value: green``


Replace the pod:
```bash
student-node ~ ➜ kubectl replace -f /tmp/color-app-cka13-arch.yaml --force

pod "color-app-cka13-arch" deleted
pod/color-app-cka13-arch replaced
```

```bash

```

```bash

```

```bash

```

```bash

```

```bash

```

```bash
```