## 1 SECTION: TROUBLESHOOTING

For this question, please set the context to `cluster1` by running:
```bash
kubectl config use-context cluster1
```
A template to create a Kubernetes pod is stored at `/root/red-probe-cka12-trb.yaml` on the `student-node`. However, using this template as-is is resulting in an error.


Fix the issue with this template and use it to create the pod. Once created, watch the pod for a minute or two to make sure its stable i.e, it's not crashing or restarting.


Make sure you do not update the `args:` section of the template.

Solution
**Try to apply the template**
```bash
kubectl apply -f red-probe-cka12-trb.yaml 
```
From the error you can see that the error is for liveness probe, so let's open the template to find out:
```bash
vi red-probe-cka12-trb.yaml
```
Under livenessProbe: you will see the type is httpGet however the rest of the options are command based so this probe should be of exec type.

Change httpGet to exec

**Try to apply the template now**
```bash
kubectl apply -f red-probe-cka12-trb.yaml 
```
Cool it worked, now let's watch the POD status, after few seconds you will notice that POD is restarting. So let's check the logs/events

```bash
kubectl get event --field-selector involvedObject.name=red-probe-cka12-trb
```
You will see an error like:
```bash
21s         Warning   Unhealthy   pod/red-probe-cka12-trb   Liveness probe failed: cat: can't open '/healthcheck': No such file or directory
```
So seems like Liveness probe is failing, lets look into it:
```bash
vi red-probe-cka12-trb.yaml
```
Notice the command `- sleep 3 ; touch /healthcheck; sleep 30;sleep 30000` it starts with a delay of `3` seconds, but the liveness probe `initialDelaySeconds` is set to `1` and `failureThreshold` is also `1`. Which means the POD will fail just after first attempt of liveness check which will happen just after 1 second of pod start. So to make it stable we must increase the `initialDelaySeconds` to at least `5`
```bash
vi red-probe-cka12-trb.yaml
```
Change `initialDelaySeconds` from `1` to `5 `and save apply the changes.
Delete old pod:
```bash
kubectl delete pod red-probe-cka12-trb
```
Apply changes:
```bash
kubectl apply -f red-probe-cka12-trb.yaml
```

## 2 SECTION: TROUBLESHOOTING
For this question, please set the context to `cluster1` by running:
```bash
kubectl config use-context cluster1
```
The `blue-dp-cka09-trb` deployment is having `0` out of `1` pods running. Fix the issue to make sure that pod is up and running.

Solution
**List the pods**
```bash
kubectl get pod
```
Most probably you see `Init:Error` or `Init:CrashLoopBackOff` for the corresponding pod.

**Look into the logs**
```bash
kubectl logs blue-dp-cka09-trb-xxxx -c init-container
```
You will see an error something like
```bash
sh: can't open 'echo 'Welcome!'': No such file or directory
```
**Edit the deployment**
```bash
kubectl edit deploy blue-dp-cka09-trb
```
Under `initContainers:` -> `- command:` add `-c` to the next line of - sh, so final command should look like this
```yaml
   initContainers:
   - command:
     - sh
     - -c
     - echo 'Welcome!'
```
If you will check pod then it must be failing again but with different error this time, let's find that out
```bash
kubectl get event --field-selector involvedObject.name=blue-dp-cka09-trb-xxxxx
```
You will see an error something like
```bash
Warning   Failed      pod/blue-dp-cka09-trb-69dd844f76-rv9z8   Error: failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error mounting "/var/lib/kubelet/pods/98182a41-6d6d-406a-a3e2-37c33036acac/volumes/kubernetes.io~configmap/nginx-config" to rootfs at "/etc/nginx/nginx.conf": mount /var/lib/kubelet/pods/98182a41-6d6d-406a-a3e2-37c33036acac/volumes/kubernetes.io~configmap/nginx-config:/etc/nginx/nginx.conf (via /proc/self/fd/6), flags: 0x5001: not a directory: unknown
```
**Edit the deployment again**

```bash
kubectl edit deploy blue-dp-cka09-trb
```
Under `volumeMounts:` -> - `mountPath: /etc/nginx/nginx.conf` -> `name: nginx-config` add `subPath: nginx.conf` and save the changes.
Finally the pod should be in running state.

## 3 SECTION: TROUBLESHOOTING
For this question, please set the context to `cluster1` by running:

```bash
kubectl config use-context cluster1
```
A pod called `check-time-cka03-trb` is continuously crashing. Figure out what is causing this and fix it.

Make sure that the `check-time-cka03-trb`POD is in `running` state.

This pod prints the current date and time at a pre-defined frequency and saves it to a file. Ensure that it continues this operation once you have fixed it.

Solution
**Look into the POD logs**
```bash
kubectl logs -f check-time-cka03-trb
```
You may not see any logs so look into the kubernetes events for `check-time-cka03-trb` POD

**Look into the POD events**
```bash
kubectl get event --field-selector involvedObject.name=check-time-cka03-trb
```
You will see an error something like:
```bash
Error: failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: exec: "/bin/bash": stat /bin/bash: no such file or directory: unknown
```
From the error we can see that its not able to execute `/bin/bash` so let's try `/bin/sh`

**Edit the pod**
```bash
kubectl get pod check-time-cka03-trb -o=yaml > check-time-cka03-trb.yaml
```
** Make the required changes in **`check-time-cka03-trb.yaml` template
```bash
vi check-time-cka03-trb.yaml
```
Under `spec:` -> `containers:` -> `command:` change `/bin/bash` to `/bin/sh `and save the file.
**Delete the old pod.**
```bash
kubectl delete pod check-time-cka03-trb
```
**Apply the updated template**
```bash
kubectl apply -f check-time-cka03-trb.yaml
```

## 4 SECTION: TROUBLESHOOTING
For this question, please set the context to `cluster1` by running:
```bash
kubectl config use-context cluster1
```
There is a Cronjob called `orange-cron-cka10-trb` which is supposed to run every two minutes (i.e 13:02, 13:04, 13:06…14:02, 14:04…and so on). This cron targets the application running inside the `orange-app-cka10-trb` pod to make sure the app is accessible. The application has been exposed internally as a ClusterIP service.


However, this cron is not running as per the expected schedule and is not running as intended.


Make the appropriate changes so that the cronjob runs as per the required schedule and it passes the accessibility checks every-time.

Solution
**Check the cron schedule**
```bash
kubectl get cronjob
```
Make sure the schedule for `orange-cron-cka10-trb` crontjob is set to `*/2 * * * *` if not then edit it.

Also before that look for the issues why this cron is failing

```bash
kubectl logs orange-cron-cka10-trb-xxxx
```
You will see some error like
```bash
curl: (6) Could not resolve host: orange-app-cka10-trb
```
You will notice that the curl is trying to hit `orange-app-cka10-trb` directly but it is supposed to hit the relevant service which is `orange-svc-cka10-trb` so we need to fix the curl command.

**Edit the cronjob**
```bash
kubectl edit cronjob orange-cron-cka10-trb
```
Change schedule `* * * * *` to `*/2 * * * *`
Change command `curl orange-app-cka10-trb` to `curl orange-svc-cka10-trb`
Wait for 2 minutes to run again this cron and it should complete now.

## 5 SECTION: TROUBLESHOOTING

For this question, please set the context to `cluster4` by running:
```bash
kubectl config use-context cluster4
```
There is a pod called `pink-pod-cka16-trb` created in the `default` namespace in `cluster4`. This app runs on port `tcp/5000` and it is exposed to end-users using an ingress resource called `pink-ing-cka16-trb` in such a way that it is supposed to be accessible using the command: `curl http://kodekloud-pink.app` on `cluster4-controlplane` host.


However, this is not working. Troubleshoot and fix this issue, making any necessary to the objects.



`Note:` You should be able to ssh into the `cluster4-controlplane` using `ssh cluster4-controlplane` command.

Solution

**SSH into the `cluster4-controlplane` host and try to access the app.**
```bash
ssh cluster4-controlplane
curl kodekloud-pink.app
```
You must be getting `503 Service Temporarily Unavailabl` error.
Let's look into the service:

```bash
kubectl edit svc pink-svc-cka16-trb
```
Under `ports:` change `protocol: UDP` to `protocol: TCP`

Try to access the app again
```bash
curl kodekloud-pink.app
```
You must be getting `curl: (6) Could not resolve host: example.com` error, from the error we can see that its not able to resolve `example.com` host which indicated that it can be some issue related to the DNS. As we know `CoreDNS` is a DNS server that can serve as the Kubernetes cluster DNS, so it can be something related to `CoreDNS`.

Let's check if we have `CoreDNS` deployment running:

```bash
kubectl get deploy -n kube-system
```
You will see that for `coredns` all relicas are down, you will see `0/0` ready pods. So let's scale up this deployment.
```bash
kubectl scale --replicas=2 deployment coredns -n kube-system
```
Once `CoreDBS` is up let's try to access to app again.
```bash
curl kodekloud-pink.ap
```
It should work now.

## 6. SECTION: TROUBLESHOOTING

For this question, please set the context to cluster1 by running:

```bash
kubectl config use-context cluster1
```
A new service account called thor-cka24-trb has been created in cluster1. Using this service account, we are trying to list and get the pods and secrets deployed in default namespace. However, this service account is not able to perform these operations.


Look into the issue and apply the appropriate fix(es) so that the service account thor-cka24-trb can perform these operations.

Solution: 

Check if this service account is associated with any role binding
```bash
kubectl get rolebinding -o yaml | grep -B 5 -A 5 thor-cka24-trb

```
ou will see role-cka24-trb is associated with this SA. So let's edit it to see the permissions
```bash
kubectl edit role role-cka24-trb
```

Update it so that resourcesand verbs section look as below:
```bash
resources:
- pods
- secrets
verbs:
- list
- get
```

## 7. SECTION: TROUBLESHOOTING

For this question, please set the context to cluster1 by running:

```bash
kubectl config use-context cluster1
```

There is an existing persistent volume called ``orange-pv-cka13-trb``. A persistent volume claim called ``orange-pvc-cka13-trb`` is created to claim storage from ``orange-pv-cka13-trb``.


However, this PVC is stuck in a ``Pending`` state. As of now, there is no data in the volume.


Troubleshoot and fix this issue, making sure that ``orange-pvc-cka13-trb`` PVC is in `` Bound`` state.

Solution: 

**List the PVC to check its status**
```bash
kubectl  get pvc
```

So we can see ``orange-pvc-cka13-trb`` PVC is in ``Pending`` state and its requesting a storage of ``150Mi``. Let's look into the events
```bash
kubectl get events --sort-by='.metadata.creationTimestamp' -A
```
You will see some errors as below:
```bash
Warning   VolumeMismatch            persistentvolumeclaim/orange-pvc-cka13-trb          Cannot bind to requested volume "orange-pv-cka13-trb": requested PV is too small
```
Let's look into ``orange-pv-cka13-trb`` volume
```bash
kubectl get pv
```
We can see that ``orange-pv-cka13-trb`` volume is of ``100Mi`` capacity which means its too small to request ``150Mi`` of storage.
Let's edit ``orange-pvc-cka13-trb ``PVC to adjust the storage requested.
```bash
kubectl get pvc orange-pvc-cka13-trb -o yaml > /tmp/orange-pvc-cka13-trb.yaml
vi /tmp/orange-pvc-cka13-trb.yaml
```
Under ``resources``: -> ``requests``: -> ``storage``: change ``150Mi`` to ``100Mi`` and save.
Delete old PVC and apply the change:
```bash
kubectl delete pvc orange-pvc-cka13-trb
kubectl apply -f /tmp/orange-pvc-cka13-trb.yaml 
```

## 8. SECTION: TROUBLESHOOTING
For this question, please set the context to cluster1 by running:
```bash
kubectl config use-context cluster1
```
There is a deployment called ``nodeapp-dp-cka08-trb`` created in the ``default`` namespace on`` cluster1``. This app is using an ingress resource named ``nodeapp-ing-cka08-trb``.


From ``cluster1-controlplane`` host we should be able to access this app using the command: ``curl http://kodekloud-ingress.app``. However, it is not working at the moment. Troubleshoot and fix the issue.



``Note:`` You should be able to ssh into the ``cluster1-controlplane`` using s``sh cluster1-controlplane`` command.

Solution:

**SSh into cluster1-controlplane**
```bash
ssh cluster1-controlplane
```
Try to access the app using ``curl http://kodekloud-ingress.app`` command. You will see ``404 Not Found error``.

Look into the ingress to make sure its configued properly.
```bash
kubectl get ingress
kubectl edit ingress nodeapp-ing-cka08-trb
```
Under ``rules``: -> ``host``: change ``example.com`` to ``kodekloud-ingress.app``
Under ``backend``: -> ``service``: ->`` name``: Change ``example-service`` to ``nodeapp-svc-cka08-trb``
Change ``port``: -> ``number``: from ``80`` to ``3000``
You should be able to access the app using ``curl http://kodekloud-ingress.app`` command now.

## 9. SECTION: TROUBLESHOOTING
For this question, please set the context to ``cluster4`` by running:
```bash
kubectl config use-context cluster4
```
The controlplane node called ``cluster4-controlplane`` in the ``cluster4`` cluster is planned for a regular maintenance. In preparation for this maintenance work, we need to take backups of this cluster. However, something is broken at the moment!


Troubleshoot the issues and take a snapshot of the ``ETCD`` database using the ``etcdctl`` utility at the location ``/opt/etcd-boot-cka18-trb.db``.


``Note``: Make sure ``etcd`` listens at its default port. Also you can ``SSH`` to the ``cluster4-controlplane`` host using the ssh ``cluster4-controlplane`` command from the ``student-node``.

Solution

SSH into cluster4-controlplane host.
```bash
ssh cluster4-controlplane
```
It might stuck for forever, let's see why that would happen. Try to list the PODs first
```bash
kubectl get pod -A
```
There might an error like
```bash
The connection to the server cluster4-controlplane:6443 was refused - did you specify the right host or port?
```
There seems to be some issue with the cluster so let's look into the logs
```bash
journalctl -u kubelet -f
```
You will see a lot of connect: connection refused erros but that must be because the different cluster components are not able to connect to the api server so try to filter out these logs to look more closly
```bash
journalctl -u kubelet -f | grep -v 'connect: connection refused'
```
You will see a lot of ``connect: connection refused`` erros but that must be because the different cluster components are not able to connect to the api server so try to filter out these logs to look more closly
```bash
journalctl -u kubelet -f | grep -v 'connect: connection refused'
```
You should see some erros as below
```bash
cluster4-controlplane kubelet[2240]: E0923 04:38:15.630925    2240 file.go:187] "Could not process manifest file" err="invalid pod: [spec.containers[0].volumeMounts[1].name: Not found: \"etcd-cert\"]" path="/etc/kubernetes/manifests/etcd.yaml"
```
So seems like there is some incorrect volume which ``etcd`` is trying to mount, let's look into the ``etcd`` manifest.
```bash
vi /etc/kubernetes/manifests/etcd.yaml 
```
Search for ``etcd-cert``, you will notice that the volume name is ````etcd-certs```` but the volume mount is trying to mount ``etcd-cert`` volume which is incorrect. Fix the volume mount name and save the changes. Let's restart ``kubelet`` service after that.
```bash
systemctl restart kubelet
```
Wait for few minutes to see if its good now.
```bash
kubectl get pod -A
```
You should be able to list the PODs now, let's try to take ``etcd`` backup now:
```bash
ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key snapshot save /opt/etcd-boot-cka18-trb.db
```
It should work now.

## 10. SECTION: TROUBLESHOOTING
For this question, please set the context to ``cluster2`` by running
```bash
kubectl config use-context cluster2
```
The ``yello-cka20-trb`` pod is stuck in a ``Pending`` state. Fix this issue and get it to a ``running`` state. Recreate the pod if necessary.

Do not ``remove`` any of the existing`` taints ``that are set on the cluster nodes.

Solution:

**Let's check the POD status**
```bash
kubectl get pod --context=cluster2
```
So you will see that ``yello-cka20-trb`` pod is in ``Pending`` state. Let's check out the relevant events.
```bash
kubectl get event --field-selector involvedObject.name=yello-cka20-trb --context=cluster2
```
You will see some errors like:
```bash
Warning   FailedScheduling   pod/yello-cka20-trb   0/2 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 1 node(s) had untolerated taint {node: node01}. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling.
```
Notice this error ``1 node(s) had untolerated taint {node: node01}`` so we can see that one of nodes have taints applied. We don't want to remove the node taints and we are not going to re-create the POD so let's look into the POD config if its using any other toleration settings.
```bash
kubectl get pod yello-cka20-trb --context=cluster2 -o yaml
```
You will notice this in the output
```yaml
tolerations:
  - effect: NoSchedule
    key: node
    operator: Equal
    value: cluster2-node01
```
Here notice that the value for key node is cluster2-node01 but the node has different value applied i.e node01 so let's update the taints values for the node as needed.
```bash
kubectl --context=cluster2 taint nodes cluster2-node01 node=cluster2-node01:NoSchedule --overwrite=true
```
Let's check the POD status again
```bash
kubectl get pod --context=cluster2
```
It should be in Running state now.
```bash

```

```bash

```

```bash

```

```bash
```

```bash

```

```bash

```

```bash

```

```bash
```

```bash

```

```bash

```

```bash

```

```bash
```

```bash

```

```bash

```

```bash

```

```bash
```

```bash

```

```bash

```

```bash

```

```bash
```

```bash

```

```bash

```

```bash

```

```bash
```

```bash

```

```bash

```

```bash

```

```bash
```