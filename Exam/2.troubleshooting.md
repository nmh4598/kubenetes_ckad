## 1 SECTION: TROUBLESHOOTING

For this question, please set the context to `cluster1` by running:
```bash
kubectl config use-context cluster1
```
A template to create a Kubernetes pod is stored at `/root/red-probe-cka12-trb.yaml` on the `student-node`. However, using this template as-is is resulting in an error.


Fix the issue with this template and use it to create the pod. Once created, watch the pod for a minute or two to make sure its stable i.e, it's not crashing or restarting.


Make sure you do not update the `args:` section of the template.

Solution
**Try to apply the template**
```bash
kubectl apply -f red-probe-cka12-trb.yaml 
```
From the error you can see that the error is for liveness probe, so let's open the template to find out:
```bash
vi red-probe-cka12-trb.yaml
```
Under livenessProbe: you will see the type is httpGet however the rest of the options are command based so this probe should be of exec type.

Change httpGet to exec

**Try to apply the template now**
```bash
kubectl apply -f red-probe-cka12-trb.yaml 
```
Cool it worked, now let's watch the POD status, after few seconds you will notice that POD is restarting. So let's check the logs/events

```bash
kubectl get event --field-selector involvedObject.name=red-probe-cka12-trb
```
You will see an error like:
```bash
21s         Warning   Unhealthy   pod/red-probe-cka12-trb   Liveness probe failed: cat: can't open '/healthcheck': No such file or directory
```
So seems like Liveness probe is failing, lets look into it:
```bash
vi red-probe-cka12-trb.yaml
```
Notice the command `- sleep 3 ; touch /healthcheck; sleep 30;sleep 30000` it starts with a delay of `3` seconds, but the liveness probe `initialDelaySeconds` is set to `1` and `failureThreshold` is also `1`. Which means the POD will fail just after first attempt of liveness check which will happen just after 1 second of pod start. So to make it stable we must increase the `initialDelaySeconds` to at least `5`
```bash
vi red-probe-cka12-trb.yaml
```
Change `initialDelaySeconds` from `1` to `5 `and save apply the changes.
Delete old pod:
```bash
kubectl delete pod red-probe-cka12-trb
```
Apply changes:
```bash
kubectl apply -f red-probe-cka12-trb.yaml
```

## 2 SECTION: TROUBLESHOOTING
For this question, please set the context to `cluster1` by running:
```bash
kubectl config use-context cluster1
```
The `blue-dp-cka09-trb` deployment is having `0` out of `1` pods running. Fix the issue to make sure that pod is up and running.

Solution
**List the pods**
```bash
kubectl get pod
```
Most probably you see `Init:Error` or `Init:CrashLoopBackOff` for the corresponding pod.

**Look into the logs**
```bash
kubectl logs blue-dp-cka09-trb-xxxx -c init-container
```
You will see an error something like
```bash
sh: can't open 'echo 'Welcome!'': No such file or directory
```
**Edit the deployment**
```bash
kubectl edit deploy blue-dp-cka09-trb
```
Under `initContainers:` -> `- command:` add `-c` to the next line of - sh, so final command should look like this
```yaml
   initContainers:
   - command:
     - sh
     - -c
     - echo 'Welcome!'
```
If you will check pod then it must be failing again but with different error this time, let's find that out
```bash
kubectl get event --field-selector involvedObject.name=blue-dp-cka09-trb-xxxxx
```
You will see an error something like
```bash
Warning   Failed      pod/blue-dp-cka09-trb-69dd844f76-rv9z8   Error: failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error mounting "/var/lib/kubelet/pods/98182a41-6d6d-406a-a3e2-37c33036acac/volumes/kubernetes.io~configmap/nginx-config" to rootfs at "/etc/nginx/nginx.conf": mount /var/lib/kubelet/pods/98182a41-6d6d-406a-a3e2-37c33036acac/volumes/kubernetes.io~configmap/nginx-config:/etc/nginx/nginx.conf (via /proc/self/fd/6), flags: 0x5001: not a directory: unknown
```
**Edit the deployment again**

```bash
kubectl edit deploy blue-dp-cka09-trb
```
Under `volumeMounts:` -> - `mountPath: /etc/nginx/nginx.conf` -> `name: nginx-config` add `subPath: nginx.conf` and save the changes.
Finally the pod should be in running state.

## 3 SECTION: TROUBLESHOOTING
For this question, please set the context to `cluster1` by running:

```bash
kubectl config use-context cluster1
```
A pod called `check-time-cka03-trb` is continuously crashing. Figure out what is causing this and fix it.

Make sure that the `check-time-cka03-trb`POD is in `running` state.

This pod prints the current date and time at a pre-defined frequency and saves it to a file. Ensure that it continues this operation once you have fixed it.

Solution
**Look into the POD logs**
```bash
kubectl logs -f check-time-cka03-trb
```
You may not see any logs so look into the kubernetes events for `check-time-cka03-trb` POD

**Look into the POD events**
```bash
kubectl get event --field-selector involvedObject.name=check-time-cka03-trb
```
You will see an error something like:
```bash
Error: failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: exec: "/bin/bash": stat /bin/bash: no such file or directory: unknown
```
From the error we can see that its not able to execute `/bin/bash` so let's try `/bin/sh`

**Edit the pod**
```bash
kubectl get pod check-time-cka03-trb -o=yaml > check-time-cka03-trb.yaml
```
** Make the required changes in **`check-time-cka03-trb.yaml` template
```bash
vi check-time-cka03-trb.yaml
```
Under `spec:` -> `containers:` -> `command:` change `/bin/bash` to `/bin/sh `and save the file.
**Delete the old pod.**
```bash
kubectl delete pod check-time-cka03-trb
```
**Apply the updated template**
```bash
kubectl apply -f check-time-cka03-trb.yaml
```

## 4 SECTION: TROUBLESHOOTING
For this question, please set the context to `cluster1` by running:
```bash
kubectl config use-context cluster1
```
There is a Cronjob called `orange-cron-cka10-trb` which is supposed to run every two minutes (i.e 13:02, 13:04, 13:06…14:02, 14:04…and so on). This cron targets the application running inside the `orange-app-cka10-trb` pod to make sure the app is accessible. The application has been exposed internally as a ClusterIP service.


However, this cron is not running as per the expected schedule and is not running as intended.


Make the appropriate changes so that the cronjob runs as per the required schedule and it passes the accessibility checks every-time.

Solution
**Check the cron schedule**
```bash
kubectl get cronjob
```
Make sure the schedule for `orange-cron-cka10-trb` crontjob is set to `*/2 * * * *` if not then edit it.

Also before that look for the issues why this cron is failing

```bash
kubectl logs orange-cron-cka10-trb-xxxx
```
You will see some error like
```bash
curl: (6) Could not resolve host: orange-app-cka10-trb
```
You will notice that the curl is trying to hit `orange-app-cka10-trb` directly but it is supposed to hit the relevant service which is `orange-svc-cka10-trb` so we need to fix the curl command.

**Edit the cronjob**
```bash
kubectl edit cronjob orange-cron-cka10-trb
```
Change schedule `* * * * *` to `*/2 * * * *`
Change command `curl orange-app-cka10-trb` to `curl orange-svc-cka10-trb`
Wait for 2 minutes to run again this cron and it should complete now.

## 5 SECTION: TROUBLESHOOTING

For this question, please set the context to `cluster4` by running:
```bash
kubectl config use-context cluster4
```
There is a pod called `pink-pod-cka16-trb` created in the `default` namespace in `cluster4`. This app runs on port `tcp/5000` and it is exposed to end-users using an ingress resource called `pink-ing-cka16-trb` in such a way that it is supposed to be accessible using the command: `curl http://kodekloud-pink.app` on `cluster4-controlplane` host.


However, this is not working. Troubleshoot and fix this issue, making any necessary to the objects.



`Note:` You should be able to ssh into the `cluster4-controlplane` using `ssh cluster4-controlplane` command.

Solution

**SSH into the `cluster4-controlplane` host and try to access the app.**
```bash
ssh cluster4-controlplane
curl kodekloud-pink.app
```
You must be getting `503 Service Temporarily Unavailabl` error.
Let's look into the service:

```bash
kubectl edit svc pink-svc-cka16-trb
```
Under `ports:` change `protocol: UDP` to `protocol: TCP`

Try to access the app again
```bash
curl kodekloud-pink.app
```
You must be getting `curl: (6) Could not resolve host: example.com` error, from the error we can see that its not able to resolve `example.com` host which indicated that it can be some issue related to the DNS. As we know `CoreDNS` is a DNS server that can serve as the Kubernetes cluster DNS, so it can be something related to `CoreDNS`.

Let's check if we have `CoreDNS` deployment running:

```bash
kubectl get deploy -n kube-system
```
You will see that for `coredns` all relicas are down, you will see `0/0` ready pods. So let's scale up this deployment.
```bash
kubectl scale --replicas=2 deployment coredns -n kube-system
```
Once `CoreDBS` is up let's try to access to app again.
```bash
curl kodekloud-pink.ap
```
It should work now.

## 6. SECTION: TROUBLESHOOTING

For this question, please set the context to cluster1 by running:

```bash
kubectl config use-context cluster1
```
A new service account called thor-cka24-trb has been created in cluster1. Using this service account, we are trying to list and get the pods and secrets deployed in default namespace. However, this service account is not able to perform these operations.


Look into the issue and apply the appropriate fix(es) so that the service account thor-cka24-trb can perform these operations.

Solution: 

Check if this service account is associated with any role binding
```bash
kubectl get rolebinding -o yaml | grep -B 5 -A 5 thor-cka24-trb

```
ou will see role-cka24-trb is associated with this SA. So let's edit it to see the permissions
```bash
kubectl edit role role-cka24-trb
```

Update it so that resourcesand verbs section look as below:
```bash
resources:
- pods
- secrets
verbs:
- list
- get
```

## 7. SECTION: TROUBLESHOOTING

For this question, please set the context to cluster1 by running:

```bash
kubectl config use-context cluster1
```

There is an existing persistent volume called ``orange-pv-cka13-trb``. A persistent volume claim called ``orange-pvc-cka13-trb`` is created to claim storage from ``orange-pv-cka13-trb``.


However, this PVC is stuck in a ``Pending`` state. As of now, there is no data in the volume.


Troubleshoot and fix this issue, making sure that ``orange-pvc-cka13-trb`` PVC is in `` Bound`` state.

Solution: 

**List the PVC to check its status**
```bash
kubectl  get pvc
```

So we can see ``orange-pvc-cka13-trb`` PVC is in ``Pending`` state and its requesting a storage of ``150Mi``. Let's look into the events
```bash
kubectl get events --sort-by='.metadata.creationTimestamp' -A
```
You will see some errors as below:
```bash
Warning   VolumeMismatch            persistentvolumeclaim/orange-pvc-cka13-trb          Cannot bind to requested volume "orange-pv-cka13-trb": requested PV is too small
```
Let's look into ``orange-pv-cka13-trb`` volume
```bash
kubectl get pv
```
We can see that ``orange-pv-cka13-trb`` volume is of ``100Mi`` capacity which means its too small to request ``150Mi`` of storage.
Let's edit ``orange-pvc-cka13-trb ``PVC to adjust the storage requested.
```bash
kubectl get pvc orange-pvc-cka13-trb -o yaml > /tmp/orange-pvc-cka13-trb.yaml
vi /tmp/orange-pvc-cka13-trb.yaml
```
Under ``resources``: -> ``requests``: -> ``storage``: change ``150Mi`` to ``100Mi`` and save.
Delete old PVC and apply the change:
```bash
kubectl delete pvc orange-pvc-cka13-trb
kubectl apply -f /tmp/orange-pvc-cka13-trb.yaml 
```

## 8. SECTION: TROUBLESHOOTING
For this question, please set the context to cluster1 by running:
```bash
kubectl config use-context cluster1
```
There is a deployment called ``nodeapp-dp-cka08-trb`` created in the ``default`` namespace on`` cluster1``. This app is using an ingress resource named ``nodeapp-ing-cka08-trb``.


From ``cluster1-controlplane`` host we should be able to access this app using the command: ``curl http://kodekloud-ingress.app``. However, it is not working at the moment. Troubleshoot and fix the issue.



``Note:`` You should be able to ssh into the ``cluster1-controlplane`` using s``sh cluster1-controlplane`` command.

Solution:

**SSh into cluster1-controlplane**
```bash
ssh cluster1-controlplane
```
Try to access the app using ``curl http://kodekloud-ingress.app`` command. You will see ``404 Not Found error``.

Look into the ingress to make sure its configued properly.
```bash
kubectl get ingress
kubectl edit ingress nodeapp-ing-cka08-trb
```
Under ``rules``: -> ``host``: change ``example.com`` to ``kodekloud-ingress.app``
Under ``backend``: -> ``service``: ->`` name``: Change ``example-service`` to ``nodeapp-svc-cka08-trb``
Change ``port``: -> ``number``: from ``80`` to ``3000``
You should be able to access the app using ``curl http://kodekloud-ingress.app`` command now.

## 9. SECTION: TROUBLESHOOTING
For this question, please set the context to ``cluster4`` by running:
```bash
kubectl config use-context cluster4
```
The controlplane node called ``cluster4-controlplane`` in the ``cluster4`` cluster is planned for a regular maintenance. In preparation for this maintenance work, we need to take backups of this cluster. However, something is broken at the moment!


Troubleshoot the issues and take a snapshot of the ``ETCD`` database using the ``etcdctl`` utility at the location ``/opt/etcd-boot-cka18-trb.db``.


``Note``: Make sure ``etcd`` listens at its default port. Also you can ``SSH`` to the ``cluster4-controlplane`` host using the ssh ``cluster4-controlplane`` command from the ``student-node``.

Solution

SSH into cluster4-controlplane host.
```bash
ssh cluster4-controlplane
```
It might stuck for forever, let's see why that would happen. Try to list the PODs first
```bash
kubectl get pod -A
```
There might an error like
```bash
The connection to the server cluster4-controlplane:6443 was refused - did you specify the right host or port?
```
There seems to be some issue with the cluster so let's look into the logs
```bash
journalctl -u kubelet -f
```
You will see a lot of connect: connection refused erros but that must be because the different cluster components are not able to connect to the api server so try to filter out these logs to look more closly
```bash
journalctl -u kubelet -f | grep -v 'connect: connection refused'
```
You will see a lot of ``connect: connection refused`` erros but that must be because the different cluster components are not able to connect to the api server so try to filter out these logs to look more closly
```bash
journalctl -u kubelet -f | grep -v 'connect: connection refused'
```
You should see some erros as below
```bash
cluster4-controlplane kubelet[2240]: E0923 04:38:15.630925    2240 file.go:187] "Could not process manifest file" err="invalid pod: [spec.containers[0].volumeMounts[1].name: Not found: \"etcd-cert\"]" path="/etc/kubernetes/manifests/etcd.yaml"
```
So seems like there is some incorrect volume which ``etcd`` is trying to mount, let's look into the ``etcd`` manifest.
```bash
vi /etc/kubernetes/manifests/etcd.yaml 
```
Search for ``etcd-cert``, you will notice that the volume name is ````etcd-certs```` but the volume mount is trying to mount ``etcd-cert`` volume which is incorrect. Fix the volume mount name and save the changes. Let's restart ``kubelet`` service after that.
```bash
systemctl restart kubelet
```
Wait for few minutes to see if its good now.
```bash
kubectl get pod -A
```
You should be able to list the PODs now, let's try to take ``etcd`` backup now:
```bash
ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key snapshot save /opt/etcd-boot-cka18-trb.db
```
It should work now.

## 10. SECTION: TROUBLESHOOTING
For this question, please set the context to ``cluster2`` by running
```bash
kubectl config use-context cluster2
```
The ``yello-cka20-trb`` pod is stuck in a ``Pending`` state. Fix this issue and get it to a ``running`` state. Recreate the pod if necessary.

Do not ``remove`` any of the existing`` taints ``that are set on the cluster nodes.

Solution:

**Let's check the POD status**
```bash
kubectl get pod --context=cluster2
```
So you will see that ``yello-cka20-trb`` pod is in ``Pending`` state. Let's check out the relevant events.
```bash
kubectl get event --field-selector involvedObject.name=yello-cka20-trb --context=cluster2
```
You will see some errors like:
```bash
Warning   FailedScheduling   pod/yello-cka20-trb   0/2 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 1 node(s) had untolerated taint {node: node01}. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling.
```
Notice this error ``1 node(s) had untolerated taint {node: node01}`` so we can see that one of nodes have taints applied. We don't want to remove the node taints and we are not going to re-create the POD so let's look into the POD config if its using any other toleration settings.
```bash
kubectl get pod yello-cka20-trb --context=cluster2 -o yaml
```
You will notice this in the output
```yaml
tolerations:
  - effect: NoSchedule
    key: node
    operator: Equal
    value: cluster2-node01
```
Here notice that the value for key node is cluster2-node01 but the node has different value applied i.e node01 so let's update the taints values for the node as needed.
```bash
kubectl --context=cluster2 taint nodes cluster2-node01 node=cluster2-node01:NoSchedule --overwrite=true
```
Let's check the POD status again
```bash
kubectl get pod --context=cluster2
```
It should be in Running state now.

## 11. SECTION: TROUBLESHOOTING

For this question, please set the context to ``cluster1`` by running:

```bash
kubectl config use-context cluster1
```
We deployed an app using a deployment called ``web-dp-cka06-trb``. it's using the ``httpd:latest`` image. There is a corresponding service called ``web-service-cka06-trb`` that exposes this app on the node port ``30005``. However, the app is not accessible!


Troubleshoot and fix this issue. Make sure you are able to access the app using ``curl http://kodekloud-exam.app:30005`` command.

Solution: 

**List the deployments to see if all PODs under web-dp-cka06-trb deployment are up and running.**
```bash
kubectl get deploy
```
You will notice that 0 out of 1 PODs are up, so let's look into the POD now.
```bash
kubectl get pod
```
You will notice that ``web-dp-cka06-trb-xxx`` pod is in ``Pending ``state, so let's checkout the relevant events.
```bash
kubectl get event --field-selector involvedObject.name=web-dp-cka06-trb-xxx
```
You should see some error/warning like this:
```bash
Warning   FailedScheduling   pod/web-dp-cka06-trb-76b697c6df-h78x4   0/1 nodes are available: 1 persistentvolumeclaim "web-cka06-trb" not found. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
```
Let's look into the PVCs
```bash
kubectl get pvc
```
You should see ``web-pvc-cka06-trb`` in the output but as per logs the POD was looking for ``web-cka06-trb`` PVC. Let's update the deployment to fix this.
```bash
kubectl edit deploy web-dp-cka06-trb
```
Under volumes: -> name: web-str-cka06-trb -> persistentVolumeClaim: -> claimName change ``web-cka06-trb`` to ``web-pvc-cka06-trb`` and save the changes.

Look into the POD again to make sure its running now

```bash
kubectl get pod
```
You will find that its still failing, most probably with ``ErrImagePull`` or ``ImagePullBackOff`` error. Now lets update the deployment again to make sure its using the correct image.
```bash
kubectl edit deploy web-dp-cka06-trb
```
Under spec: -> containers: -> change image from ``httpd:letest`` to ``httpd:latest ``and save the changes.
Look into the POD again to make sure its running now
```bash
kubectl get pod
```
You will notice that POD is still crashing, let's look into the POD logs.
```bash
kubectl logs web-dp-cka06-trb-xxxx
```
If there are no useful logs then look into the events
```bash
kubectl get event --field-selector involvedObject.name=web-dp-cka06-trb-xxxx --sort-by='.lastTimestamp'
```
You should see some errors/warnings as below
```bash
Warning   FailedPostStartHook   pod/web-dp-cka06-trb-67dccb7487-2bjgf   Exec lifecycle hook ([/bin -c echo 'Test Page' > /usr/local/apache2/htdocs/index.html]) for Container "web-container" in Pod "web-dp-cka06-trb-67dccb7487-2bjgf_default(4dd6565e-7f1a-4407-b3d9-ca595e6d4e95)" failed - error: rpc error: code = Unknown desc = failed to exec in container: failed to start exec "c980799567c8176db5931daa2fd56de09e84977ecd527a1d1f723a862604bd7c": OCI runtime exec failed: exec failed: unable to start container process: exec: "/bin": permission denied: unknown, message: ""
```
Let's look into the lifecycle hook of the pod
```bash
kubectl edit deploy web-dp-cka06-trb
```
Under containers: -> lifecycle: -> postStart: -> exec: -> command: change ``/bin`` to ``/bin/sh``
Look into the POD again to make sure its running now
```bash
kubectl get pod
```
Finally pod should be in running state. Let's try to access the webapp now.
```bash
curl http://kodekloud-exam.app:30005
```
You will see error ``curl: (7) Failed to connect to kodekloud-exam.app port 30005: Connection refused``
Let's look into the service
```bash
kubectl edit svc web-service-cka06-trb
```
Let's verify if the selector labels and ports are correct as needed. You will note that service is using ``selector: -> app: web-cka06-trb``
Now, let's verify the app labels:
```bash
kubectl get deploy web-dp-cka06-trb -o yaml
```
Under labels you will see ``labels: -> deploy: web-app-cka06-trb``
So we can see that service is using wrong selector label, let's edit the service to fix the same
```bash
kubectl edit svc web-service-cka06-trb
```
Let's try to access the webapp now.
```bash
curl http://kodekloud-exam.app:30005
```
## 12. SECTION: TROUBLESHOOTING

The ``cat-cka22-trb`` pod is stuck in ``Pending`` state. Look into the issue to fix the same. Make sure that the ``pod`` is in running state and its stable (i.e not restarting or crashing).


``Note``: Do not make any changes to the pod (No changes to pod config but you may destory and re-create).

Solution: 

Let's check the POD status
```bash
kubectl get pod
```
You will see that ``cat-cka22-trb`` pod is stuck in ``Pending`` state. So let's try to look into the events
```bash
kubectl --context cluster2 get event --field-selector involvedObject.name=cat-cka22-trb
```
ou will see some logs as below
```bash
Warning   FailedScheduling   pod/cat-cka22-trb   0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/2 nodes are available: 3 Preemption is not helpful for scheduling.
```
So seems like this POD is using the node affinity, let's look into the POD to understand the node affinity its using.
```bash
kubectl --context cluster2 get pod cat-cka22-trb -o yaml
```
Under ``affinity``: you will see its looking for ``key: node`` and ``values: cluster2-node02`` so let's verify if node01 has these labels applied.
```bash
kubectl --context cluster2 get node cluster2-node01 -o yaml
```
Look under ``labels:`` and you will not find any such label, so let's add this label to this node.
```bash
kubectl label node cluster1-node01 node=cluster2-node01
```
Check again the node details
```bash
kubectl get node cluster2-node01 -o yaml
```
The new label should be there, let's see if POD is scheduled now on this node
```bash
kubectl --context cluster2 get pod
```
Its is but it must be crashing or restarting, so let's look into the pod logs
```bash
kubectl --context cluster2 logs -f cat-cka22-trb
```
You will see logs as below:
```bash
The HOST variable seems incorrect, it must be set to kodekloud
```
Let's look into the POD env variables to see if there is any HOST env variable
```bash
kubectl --context cluster2 get pod -o yaml
```
Under env: you will see this
```yaml
env:
- name: HOST
  valueFrom:
    secretKeyRef:
      key: hostname
      name: cat-cka22-trb
```
So we can see that HOST variable is defined and its value is being retrieved from a secret called "cat-cka22-trb". Let's look into this secret.
```bash
kubectl --context cluster2 get secret
kubectl --context cluster2 get secret cat-cka22-trb -o yaml
```
You will find a ``key/value`` pair under`` data:``, let's try to decode it to see its value:
```bash
echo "<the decoded value you see for hostname" | base64 -d
```
ok so the value is set to ``kodekloude`` which is incorrect as it should be set to ``kodekloud``. So let's update the secret:
```bash
echo "kodekloud" | base64
kubectl edit secret cat-cka22-trb
```
Change requests storage ``hostname: a29kZWtsb3Vkdg==`` to ``hostname: a29kZWtsb3VkCg==`` (values may vary)
POD should be good now.

## 13. SECTION: TROUBLESHOOTING

The controlplane node called cluster4-controlplane in the cluster4 cluster is planned for a regular maintenance. In preparation for this maintenance work, we need to take backups of this cluster. However, something is broken at the moment!


Troubleshoot the issues and take a snapshot of the ETCD database using the etcdctl utility at the location /opt/etcd-boot-cka18-trb.db.


Note: Make sure etcd listens at its default port. Also you can SSH to the cluster4-controlplane host using the ssh cluster4-controlplane command from the student-node.

Solution: 

SSH into cluster4-controlplane host.
```bash
ssh cluster4-controlplane
```
Let's take etcd backup
```bash
ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key snapshot save /opt/etcd-boot-cka18-trb.db
```
It might stuck for forever, let's see why that would happen. Try to list the PODs first
```bash
kubectl get pod -A
```
There might an error like
```bash
The connection to the server cluster4-controlplane:6443 was refused - did you specify the right host or port?
```
There seems to be some issue with the cluster so let's look into the logs
```bash
journalctl -u kubelet -f

```
You will see a lot of connect: connection refused erros but that must be because the different cluster components are not able to connect to the api server so try to filter out these logs to look more closly
```bash
journalctl -u kubelet -f | grep -v 'connect: connection refused'
```
You should see some erros as below
```bash
cluster4-controlplane kubelet[2240]: E0923 04:38:15.630925    2240 file.go:187] "Could not process manifest file" err="invalid pod: [spec.containers[0].volumeMounts[1].name: Not found: \"etcd-cert\"]" path="/etc/kubernetes/manifests/etcd.yaml"
```
So seems like there is some incorrect volume which etcd is trying to mount, let's look into the etcd manifest.
```bash
vi /etc/kubernetes/manifests/etcd.yaml 
```
Search for etcd-cert, you will notice that the volume name is etcd-certs but the volume mount is trying to mount etcd-cert volume which is incorrect. Fix the volume mount name and save the changes. Let's restart kubelet service after that.
```bash
systemctl restart kubelet
```
Wait for few minutes to see if its good now
```bash
kubectl get pod -A
```
You should be able to list the PODs now, let's try to take etcd backup now:
```bash
ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key snapshot save /opt/etcd-boot-cka18-trb.db
```
t should work now.

## 14 . SECTION: TROUBLESHOOTING

A service account called deploy-cka19-trb is created in cluster1 along with a cluster role called deploy-cka19-trb-role. This role should have the permissions to get all the deployments under the default namespace. However, at the moment, it is not able to.


Find out what is wrong and correct it so that the deploy-cka19-trb service account is able to get deployments under default namespace.

Solution: 

Let's see if deploy-cka19-trb service account is able to get the deployments.
```bash
kubectl auth can-i get deployments --as=system:serviceaccount:default:deploy-cka19-trb
```
We can see its not since we are getting no in the output.

Let's look into the cluster role:
```bash
kubectl get clusterrole deploy-cka19-trb-role -o yaml
```
The rules would be fine but we can see that there is no cluster role binding and service account associated with this. So let's create a cluster role binding.
```bash
kubectl create clusterrolebinding deploy-cka19-trb-role-binding --clusterrole=deploy-cka19-trb-role --serviceaccount=default:deploy-cka19-trb
```
Let's see if deploy-cka19-trb service account is able to get the deployments now.
```bash
kubectl auth can-i get deployments --as=system:serviceaccount:default:deploy-cka19-trb
```

## 15. SECTION: TROUBLESHOOTING

One of the nginx based pod called ``cyan-pod-cka28-trb`` is running under ``cyan-ns-cka28-trb`` namespace and it is exposed within the cluster using ``cyan-svc-cka28-trb`` service.

This is a ``restricted`` pod so a network policy called ``cyan-np-cka28-trb`` has been created in the same namespace to apply some restrictions on this pod.


Two other pods called ``cyan-white-cka28-trb1`` and ``cyan-black-cka28-trb`` are also running in the default namespace.


The nginx based app running on the ``cyan-pod-cka28-trb`` pod is exposed internally on the default nginx port ``(80)``.


Expectation: This app should ``only`` be accessible from the ``cyan-white-cka28-trb1`` pod.


Problem: This app is not accessible from anywhere.


Troubleshoot this issue and fix the connectivity as per the requirement listed above.

Note: You can exec into ``cyan-white-cka28-trb`` and ``cyan-black-cka28-trb`` pods and test connectivity using the curl utility.


You may update the network policy, but make sure it is not deleted from the ``cyan-ns-cka28-trb`` namespace.

Solution: 

Let's look into the network policy

```bash
kubectl edit networkpolicy cyan-np-cka28-trb -n cyan-ns-cka28-trb
```
Under ``spec``: -> ``egress``: you will notice there is not ``cidr``: block has been added, since there is no restrcitions on ``egress`` traffic so we can update it as below. Further you will notice that the port used in the policy is ``8080`` but the app is running on default port which is ``80`` so let's update this as well (under ``egress`` and ``ingress``):

Change ``port: 8080`` to ``port: 80``
```yaml
- ports:
  - port: 80
    protocol: TCP
  to:
  - ipBlock:
      cidr: 0.0.0.0/0
```
Now, lastly notice that there is no POD selector has been used in ``ingress`` section but this app is supposed to be accessible from ``cyan-white-cka28-trb`` pod under ``default`` namespace. So let's edit it to look like as below:
```yaml
ingress:
- from:
  - namespaceSelector:
      matchLabels:
        kubernetes.io/metadata.name: default
   podSelector:
      matchLabels:
        app: cyan-white-cka28-trb
```
Now, let's try to access the app from ``cyan-white-pod-cka28-trb``
```bash
kubectl exec -it cyan-white-cka28-trb -- sh
curl cyan-svc-cka28-trb.cyan-ns-cka28-trb.svc.cluster.local
```
Also make sure its not accessible from the other pod(s)
```bash
kubectl exec -it cyan-black-cka28-trb -- sh
curl cyan-svc-cka28-trb.cyan-ns-cka28-trb.svc.cluster.local
```
It should not work from this pod. So its looking good now.

## 16. SECTION: TROUBLESHOOTING
A pod ``called nginx-cka01-trb`` is running in the ``default`` namespace. There is a container called ``nginx-container`` running inside this pod that uses the image ``nginx:latest``. There is another sidecar container called ``logs-container`` that runs in this pod.

For some reason, this pod is continuously crashing. Identify the issue and fix it. Make sure that the pod is in a ``running`` state and you are able to access the website using the ``curl http://kodekloud-exam.app:30001`` command on the controlplane node of ``cluster1``.

Solution: 

**Check the container logs:**
```bash
kubectl logs -f nginx-cka01-trb -c nginx-container
```
You can see that its not able to pull the image.

**Edit the pod**
```bash
kubectl edit pod nginx-cka01-trb -o yaml
```
Change image tag from ``nginx:latst`` to ``nginx:latest``
Let's check now if the POD is in Running state
```bash
kubectl get pod
```
You will notice that its still crashing, so check the logs again
```bash
kubectl logs -f nginx-cka01-trb -c nginx-container
```
From the logs you will notice that ``nginx-container`` is looking good now so it might be the sidecar container that is causing issues. Let's check its logs.
```bash
kubectl logs -f nginx-cka01-trb -c logs-container
```
You will see some logs as below
```bash
cat: can't open '/var/log/httpd/access.log': No such file or directory
cat: can't open '/var/log/httpd/error.log': No such file or directory
```
Now, let's look into the sidecar container
```bash
kubectl get pod nginx-cka01-trb -o yaml
```
Under ``containers``: check the ``command``: section, this is the command which is failing. If you notice its looking for the logs under /``var/log/httpd/`` directory but the mounted volume for logs is ``/var/log/nginx`` (under ``volumeMounts:``). So we need to fix this path:
```bash
kubectl get pod nginx-cka01-trb -o yaml > /tmp/test.yaml
vi /tmp/test.yaml
```
Under ``command:`` change ``/var/log/httpd/access.log`` and ``/var/log/httpd/error.log`` to ``/var/log/nginx/access.log`` and ``/var/log/nginx/error.log`` respectively.

Delete the existing POD now
```bash
kubectl delete pod nginx-cka01-trb
```
Create new one from the template
```bash
kubectl apply -f /tmp/test.yaml
```
Let's check now if the POD is in Running state
```bash
kubectl get pod
```
It should be good now. So let's try to access the app.
```bash
curl http://kodekloud-exam.app:30001
```
You will see error
```bash
curl: (7) Failed to connect to kodekloud-exam.app port 30001: Connection refused
```
So you are not able to access the website, et's look into the service configuration.

**Edit the service**
```bash
kubectl edit svc nginx-service-cka01-trb -o yaml 
```
Change app label under selector from ``httpd-app-cka01-trb`` to ``nginx-app-cka01-trb``
**You should be able to access the website now.**
```bash
curl http://kodekloud-exam.app:30001
```

## 17. SECTION: TROUBLESHOOTING
For this question, please set the context to ``cluster1`` by running:

```bash
kubectl config use-context cluster1
```
A YAML template for a Kubernetes deployment is stored at ``/root/app-cka07-trb.yaml``. However, creating a deployment using this file is failing. Investigate the cause of the errors and fix the issue.
Make sure that the pod is in running state once deployed.

``Note``: Do not to make any changes in the template file.

Solution:

**Try to apply the template:**
```bash
kubectl apply -f /root/app-cka07-trb.yaml
```
You will see an error something like:
```bash
Error from server (NotFound): error when creating "/root/app-cka07-trb.yaml": namespaces "app-cka07-trb" not found
Error from server (NotFound): error when creating "/root/app-cka07-trb.yaml": namespaces "app-cka07-trb" not found
```
From the error you can see that its looking for app-cka07-trb namespace so let's find out if this namespace exists:
```bash
kubectl get ns
```
You will not see this namespace in the list so let's create it.

**Create the namespace**
```bash
kubectl create ns app-cka07-trb
```
**Apply the template**
```bash
kubectl a apply -f /root/app-cka07-trb.yaml
```
**Verify the POD is up**
```bash
kubectl get pod -n app-cka07-trb
```

```bash

```

```bash

```

```bash

```

```bash

```

```bash

```

```bash

```

```bash

```

```bash

```