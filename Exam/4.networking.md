## 1. SECTION: SERVICE NETWORKING

For this question, please set the context to cluster1 by running:
```bash
kubectl config use-context cluster1
```
John is setting up a two tier application stack that is supposed to be accessible using the service ``curlme-cka01-svcn``. To test that the service is accessible, he is using a pod called ``curlpod-cka01-svcn``. However, at the moment, he is unable to get any response from the application.

Troubleshoot and fix this issue so the application stack is accessible.

While you may delete and recreate the service ``curlme-cka01-svcn``, please do not alter it in anyway.

Solution:

Test if the service ``curlme-cka01-svcn`` is accessible from pod ``curlpod-cka01-svcn`` or not.
```bash
kubectl exec curlpod-cka01-svcn -- curl curlme-cka01-svcn

.....
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:--  0:00:10 --:--:--     0
```
We did not get any response. Check if the service is properly configured or not.
```bash
kubectl describe svc curlme-cka01-svcn ''

....
Name:              curlme-cka01-svcn
Namespace:         default
Labels:            <none>
Annotations:       <none>
Selector:          run=curlme-ckaO1-svcn
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.109.45.180
IPs:               10.109.45.180
Port:              <unset>  80/TCP
TargetPort:        80/TCP
Endpoints:         <none>
Session Affinity:  None
Events:            <none>
```
The service has no endpoints configured. As we can delete the resource, let's delete the service and create the service again.

To delete the service, use the command ``kubectl delete svc curlme-cka01-svcn.``
You can create the service using imperative way or declarative way.


Using imperative command:

```bash
kubectl expose pod curlme-cka01-svcn --port=80
```
Using declarative manifest:
```bash
kubectl exec curlpod-cka01-svcn -- curl curlme-cka01-svcn
```

## 2. SECTION: SERVICE NETWORKING

For this question, please set the context to cluster1 by running:
```bash
kubectl config use-context cluster1
```
Create a nginx pod called ``nginx-resolver-cka06-svcn`` using image ``nginx``, expose it internally with a service called ``nginx-resolver-service-cka06-svcn.
``
Test that you are able to look up the service and pod names from within the cluster. Use the image: ``busybox:1.28`` for dns lookup. Record results in ``/root/CKA/nginx.svc.cka06.svcn`` and ``/root/CKA/nginx.pod.cka06.svcn``

Solution: 

Switching to cluster1:
```bash
kubectl config use-context cluster1
```
To create a pod ``nginx-resolver-cka06-svcn`` and expose it internally:

```bash
student-node ~ ➜ kubectl run nginx-resolver-cka06-svcn --image=nginx 
student-node ~ ➜ kubectl expose pod/nginx-resolver-cka06-svcn --name=nginx-resolver-service-cka06-svcn --port=80 --target-port=80 --type=ClusterIP 
```
To create a pod ``test-nslookup``. Test that you are able to look up the service and pod names from within the cluster:

```bash
student-node ~ ➜  kubectl run test-nslookup --image=busybox:1.28 --rm -it --restart=Never -- nslookup nginx-resolver-service-cka06-svcn
student-node ~ ➜  kubectl run test-nslookup --image=busybox:1.28 --rm -it --restart=Never -- nslookup nginx-resolver-service-cka06-svcn > /root/CKA/nginx.svc.cka06.svcn
```
Get the IP of the ``nginx-resolver-cka06-svcn`` pod and replace the dots(.) with hyphon(-) which will be used below.

```bash
student-node ~ ➜  kubectl get pod nginx-resolver-cka06-svcn -o wide
student-node ~ ➜  IP=`kubectl get pod nginx-resolver-cka06-svcn -o wide --no-headers | awk '{print $6}' | tr '.' '-'`
student-node ~ ➜  kubectl run test-nslookup --image=busybox:1.28 --rm -it --restart=Never -- nslookup $IP.default.pod > /root/CKA/nginx.pod.cka06.svcn
```

## 3. SECTION: SERVICE NETWORKING

For this question, please set the context to ``cluster3`` by running:
```bash
kubectl config use-context cluster3
```
Create a ``ReplicaSet`` with name ``checker-cka10-svcn`` in ``ns-12345-svcn`` namespace with image ``registry.k8s.io/e2e-test-images/jessie-dnsutils:1.3.``


Make sure to specify the below specs as well:


command ``sleep 3600``
replicas set to ``2``
container name: ``dns-image``



Once the checker pods are up and running, store the output of the command `nslookup kubernetes.default` from any one of the checker pod into the file `/root/dns-output-12345-cka10-svcn `on `student-node.`

Solution: 

Create the ``ReplicaSet`` as per the requirements:

```bash
kubectl apply -f - << EOF
---
apiVersion: v1
kind: Namespace
metadata:
  creationTimestamp: null
  name: ns-12345-svcn
spec: {}
status: {}

---
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: checker-cka10-svcn
  namespace: ns-12345-svcn
  labels:
    app: dns
    tier: testing
spec:
  replicas: 2
  selector:
    matchLabels:
      tier: testing
  template:
    metadata:
      labels:
        tier: testing
    spec:
      containers:
      - name: dns-image
        image: registry.k8s.io/e2e-test-images/jessie-dnsutils:1.3
        command:
          - sleep
          - "3600"
EOF
```
Now let's test if the ``nslookup`` command is working :
```bash
student-node ~ ➜  k get pods -n ns-12345-svcn 
NAME                       READY   STATUS    RESTARTS   AGE
checker-cka10-svcn-d2cd2   1/1     Running   0          12s
checker-cka10-svcn-qj8rc   1/1     Running   0          12s

student-node ~ ➜  POD_NAME=`k get pods -n ns-12345-svcn --no-headers | head -1 | awk '{print $1}'`

student-node ~ ➜  kubectl exec -n ns-12345-svcn -i -t $POD_NAME -- nslookup kubernetes.default
;; connection timed out; no servers could be reached

command terminated with exit code 1
```
There seems to be a problem with the name resolution. Let's check if our coredns pods are up and if any service exists to reach them:
```bash
student-node ~ ➜  k get pods -n kube-system | grep coredns
coredns-6d4b75cb6d-cprjz                        1/1     Running   0             42m
coredns-6d4b75cb6d-fdrhv                        1/1     Running   0             42m

student-node ~ ➜  k get svc -n kube-system 
NAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE
kube-dns   ClusterIP   10.96.0.10   <none>        53/UDP,53/TCP,9153/TCP   62m
```
Everything looks okay here but the name resolution problem exists, let's see if the ``kube-dns`` service have any ``active`` endpoints:
```bash
student-node ~ ➜  kubectl get ep -n kube-system kube-dns 
NAME       ENDPOINTS   AGE
kube-dns   <none>      63m
```
Finally, we have our culprit.


If we dig a little deeper, we will it is using wrong labels and selector:
```bash
student-node ~ ➜  kubectl describe svc -n kube-system kube-dns 
Name:              kube-dns
Namespace:         kube-system
....
Selector:          k8s-app=core-dns
Type:              ClusterIP
...

student-node ~ ➜  kubectl get deploy -n kube-system --show-labels | grep coredns
coredns   2/2     2   
```
Let's update the ``kube-dns`` service it to point to correct set of pods:
```bash
student-node ~ ➜  kubectl patch service -n kube-system kube-dns -p '{"spec":{"selector":{"k8s-app": "kube-dns"}}}'
service/kube-dns patched

student-node ~ ➜  kubectl get ep -n kube-system kube-dns 
NAME       ENDPOINTS                                              AGE
kube-dns   10.50.0.2:53,10.50.192.1:53,10.50.0.2:53 + 3 more...   69m
```
`NOTE`: We can use any method to update kube-dns service. In our case, we have used `kubectl patch` command.

Now let's store the correct output to `/root/dns-output-12345-cka10-svcn`:
```bash
student-node ~ ➜  kubectl exec -n ns-12345-svcn -i -t $POD_NAME -- nslookup kubernetes.default
Server:         10.96.0.10
Address:        10.96.0.10#53

Name:   kubernetes.default.svc.cluster.local
Address: 10.96.0.1


student-node ~ ➜  kubectl exec -n ns-12345-svcn -i -t $POD_NAME -- nslookup kubernetes.default > /root/dns-output-12345-cka10-svcn
```
## 4. SECTION: SERVICE NETWORKING


For this question, please set the context to cluster3 by running:

```bash
kubectl config use-context cluster3
```
Create a deployment named ``hr-web-app-cka08-svcn`` using the image ``kodekloud/webapp-color`` with 2 replicas.

Expose the ``hr-web-app-cka08-svcn`` as service ``hr-web-app-service-cka08-svcn`` application on port ``30082`` on the nodes of the cluster.

The web application listens on port ``8080``

Solution 

```bash
kubectl config use-context cluster3
```

On student-node, use the command: ``kubectl create deployment hr-web-app-cka08-svcn --image=kodekloud/webapp-color --replicas=2``

Now we can run the command: `kubectl expose deployment hr-web-app-cka08-svcn --type=NodePort --port=8080 --name=hr-web-app-service-cka08-svcn --dry-run=client -o yaml > hr-web-app-service-cka08-svcn.yaml` to generate a service definition file.


Now, in generated service definition file add the `nodePort` field with the given port number under the `ports` section and create a service.

## 5. SECTION: SERVICE NETWORKING
For this question, please set the context to cluster3 by running:
```bash
kubectl config use-context cluster3
```
Deploy a messaging-cka07-svcn pod using the redis:alpine image with the labels set to tier=msg.



Now create a service messaging-service-cka07-svcn to expose the messaging-cka07-svcn application within the cluster on port 6379.

TIP: Use imperative commands

Solution: 

Switch to cluster2 :
On student-node, use the command 
```bash
kubectl run messaging-cka07-svcn --image=redis:alpine -l tier=msg

kubectl expose pod messaging-cka07-svcn --port=6379 --name messaging-service-cka07-svcn.
```
## 6. SECTION: SERVICE NETWORKING

Create a ``loadbalancer`` service with name ``wear-service-cka09-svcn`` to expose the deployment ``webapp-wear-cka09-svcn`` application in ``app-space`` namespace.

Solution: 

```bash
student-node ~ ➜  kubectl expose -n app-space deployment webapp-wear-cka09-svcn --type=LoadBalancer --name=wear-service-cka09-svcn --port=8080
service/wear-service-cka09-svcn exposed

student-node ~ ➜  k get svc -n app-space
NAME                      TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
wear-service-cka09-svcn   LoadBalancer   10.43.68.233   172.25.0.14   8080:32109/TCP   14s
```

## 7. SECTION: SERVICE NETWORKING
Part I:



Create a ClusterIP service .i.e. service-3421-svcn which should expose the pods namely pod-23 and pod-21 with port set to 8080 and targetport to 80.



Part II:



Store the pod names and their ip addresses from all namespaces at /root/pod_ips_cka05_svcn where the output is sorted by their IP's.

Please ensure the format as shown below:
```bash
POD_NAME        IP_ADDR
pod-1           ip-1
pod-3           ip-2
pod-2           ip-3
...
```
Solution: 

Switching to cluster3:

```bash
kubectl config use-context cluster3
```
The easiest way to route traffic to a specific pod is by the use of labels and selectors . List the pods along with their labels:
```bash
student-node ~ ➜  kubectl get pods --show-labels 
NAME     READY   STATUS    RESTARTS   AGE     LABELS
pod-12   1/1     Running   0          5m21s   env=dev,mode=standard,type=external
pod-34   1/1     Running   0          5m20s   env=dev,mode=standard,type=internal
pod-43   1/1     Running   0          5m20s   env=prod,mode=exam,type=internal
pod-23   1/1     Running   0          5m21s   env=dev,mode=exam,type=external
pod-32   1/1     Running   0          5m20s   env=prod,mode=standard,type=internal
pod-21   1/1     Running   0          5m20s   env=prod,mode=exam,type=external
```
Looks like there are a lot of pods created to confuse us. But we are only concerned with the labels of pod-23 and pod-21.



As we can see both the required pods have labels mode=exam,type=external in common. Let's confirm that using kubectl too:

```bash
student-node ~ ➜  kubectl get pod -l mode=exam,type=external                                       
NAME     READY   STATUS    RESTARTS   AGE
pod-23   1/1     Running   0          9m18s
pod-21   1/1     Running   0          9m17s
```
Nice!! Now as we have figured out the labels, we can proceed further with the creation of the service:

```bash
student-node ~ ➜  kubectl create service clusterip service-3421-svcn --tcp=8080:80 --dry-run=client -o yaml > service-3421-svcn.yaml
```
Now modify the service definition with selectors as required before applying to k8s cluster:
```bash
student-node ~ ➜  cat service-3421-svcn.yaml 
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    app: service-3421-svcn
  name: service-3421-svcn
spec:
  ports:
  - name: 8080-80
    port: 8080
    protocol: TCP
    targetPort: 80
  selector:
    app: service-3421-svcn  # delete 
    mode: exam    # add
    type: external  # add
  type: ClusterIP
status:
  loadBalancer: {}
```
Finally let's apply the service definition:
```bash
student-node ~ ➜  kubectl apply -f service-3421-svcn.yaml
service/service-3421 created

student-node ~ ➜  k get ep service-3421-svcn 
NAME           ENDPOINTS                     AGE
service-3421   10.42.0.15:80,10.42.0.17:80   52s
```
To store all the pod name along with their IP's , we could use imperative command as shown below:
```bash
student-node ~ ➜  kubectl get pods -A -o=custom-columns='POD_NAME:metadata.name,IP_ADDR:status.podIP' --sort-by=.status.podIP

POD_NAME                                  IP_ADDR
helm-install-traefik-crd-lbwzr            10.42.0.2
local-path-provisioner-7b7dc8d6f5-d4x7t   10.42.0.3
metrics-server-668d979685-vh7bk           10.42.0.4
...

# store the output to /root/pod_ips
student-node ~ ➜  kubectl get pods -A -o=custom-columns='POD_NAME:metadata.name,IP_ADDR:status.podIP' --sort-by=.status.podIP > /root/pod_ips_cka05_svcn
```
## 8. SECTION: SERVICE NETWORKING
There is a deployment nginx-deployment-cka04-svcn in cluster3 which is exposed using service nginx-service-cka04-svcn.



Create an ingress resource nginx-ingress-cka04-svcn to load balance the incoming traffic with the following specifications:


pathType: Prefix and path: /

Backend Service Name: nginx-service-cka04-svcn

Backend Service Port: 80

ssl-redirect is set to false

Solution: 

First change the context to "cluster3":
```bash
student-node ~ ➜  kubectl config use-context cluster3
Switched to context "cluster3".
```
Now apply the ingress resource with the given requirements
```bash
kubectl apply -f - << EOF
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: nginx-ingress-cka04-svcn
  annotations:
    ingress.kubernetes.io/ssl-redirect: "false"
spec:
  rules:
  - http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: nginx-service-cka04-svcn
            port:
              number: 80
EOF

```
Check if the ingress resource was successfully created:

```bash
student-node ~ ➜  kubectl get ingress
NAME                       CLASS    HOSTS   ADDRESS       PORTS   AGE
nginx-ingress-cka04-svcn 
```
As the ingress controller is exposed on cluster3-controlplane using traefik service, we need to ssh to cluster3-controlplane first to check if the ingress resource works properly:
```bash
student-node ~ ➜  ssh cluster3-controlplane

cluster3-controlplane:~# curl -I 172.25.0.11
HTTP/1.1 200 OK
...
```

## 9. SECTION: SERVICE NETWORKING
We have an external webserver running on ``student-nod``e which is exposed at port`` 9999``. We have created a service called ``external-webserver-cka03-svcn`` that can connect to our local webserver from within the kubernetes cluster3 but at the moment it is not working as expected.



Fix the issue so that other pods within cluster3 can use external-webserver-cka03-svcn service to access the webserver.

Solution:

```bash
student-node ~ ➜  curl student-node:9999
...
<h1>Welcome to nginx!</h1>
...
```
Now we will check if service is correctly defined:
```bash
student-node ~ ➜  kubectl describe svc external-webserver-cka03-svcn 
Name:              external-webserver-cka03-svcn
Namespace:         default
.
.
Endpoints:         <none> # there are no endpoints for the service
...
```
As we can see there is no endpoints specified for the service, hence we won't be able to get any output. Since we can not destroy any k8s object, let's create the endpoint manually for this service as shown below:

```bash
student-node ~ ➜  export IP_ADDR=$(ifconfig eth0 | grep inet | awk '{print $2}')

student-node ~ ➜ kubectl --context cluster3 apply -f - <<EOF
apiVersion: v1
kind: Endpoints
metadata:
  # the name here should match the name of the Service
  name: external-webserver-cka03-svcn
subsets:
  - addresses:
      - ip: $IP_ADDR
    ports:
      - port: 9999
EOF
```
Finally check if the ``curl test`` works now:
```bash
student-node ~ ➜  kubectl --context cluster3 run --rm  -i test-curl-pod --image=curlimages/curl --restart=Never -- curl -m 2 external-webserver-cka03-svcn
...
<title>Welcome to nginx!</title>
...
```

```bash

```

```bash

```

```bash

```

```bash

```

```bash

```

```bash

```

```bash

```

```bash

```

```bash

```

```bash

```

```bash
```
```bash

```

```bash

```

```bash

```

```bash

```

```bash

```

```bash

```

```bash

```

```bash

```

```bash

```

```bash

```

```bash

```

```bash

```

```bash

```

```bash

```

```bash
```